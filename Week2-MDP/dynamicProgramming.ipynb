{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 2 : MDPs AND DYNAMIC PROGRAMMING\n",
    "\n",
    "**Due date : 05/02/2018**\n",
    "\n",
    "**By : Julien Roy and David Kanaa**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1 - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 : \n",
    "\n",
    "Bla bla bla \n",
    "\n",
    "$$Maths + maths = Maths$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : \n",
    "\n",
    "\n",
    "Bla bla bla \n",
    "\n",
    "$$Maths + maths = Maths$$\n",
    "$$ Maths + 2 * 3 = 2x + 5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2 - PROGRAMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Bla bla bla MDP bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-states MDP\n",
    "\n",
    "* $P(s_0 | s_0, a_0) = 0.5,  r = 5$\n",
    "* $P(s_1 | s_0, a_0) = 0.5,  r = 5$\n",
    "* $P(s_0 | s_0, a_1) = 0$\n",
    "* $P(s_1 | s_0, a_1) = 1,  r = 10$\n",
    "* $P(s_1 | s_0, a_2) = 0$\n",
    "* $P(s_1 | s_1, a_2) = 1,  r = -1$\n",
    "\n",
    "\n",
    "* $\\gamma = 0.95$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_2statesworld():\n",
    "\n",
    "    P = np.zeros((2,2,3)) # P(s'|s,a) ... our model of the environment\n",
    "    P[0,0,0] = 0.5\n",
    "    P[1,0,0] = 0.5\n",
    "    P[1,0,1] = 1.\n",
    "    P[1,1,2] = 1.\n",
    "\n",
    "    R = np.zeros((2,3))  # R(s,a) ... the reward funciton \n",
    "    R[0,0] = 5\n",
    "    R[0,1] = 10\n",
    "    R[1,2] = -1\n",
    "\n",
    "    states = [0, 1]\n",
    "    actions = [[0, 1], [2]]\n",
    "    next_states = [0, 1]\n",
    "    gamma = 0.95\n",
    "    \n",
    "    return P, R, states, actions, next_states, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(P, R, states, actions, next_states, gamma, epsilon=1e-4):\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = np.zeros((2,), dtype=np.float) # V(s) ... our value function estimate for PI\n",
    "    PI = np.array([np.random.uniform(0, 2), 2], dtype=np.int)     # PI(s) ... our greedy policy\n",
    "    policy_stable = False\n",
    "    all_k = []\n",
    "    \n",
    "    print(\"INITIALIZATION\".format(len(all_k), all_k))\n",
    "    print(\"\\nV = \", np.round(V_k))\n",
    "    print(\"\\nPI = \", PI)\n",
    "    \n",
    "    while not policy_stable:\n",
    "        \n",
    "        # 2. POLICY EVALUATION (iterates until V_k converges) \n",
    "        k = 0\n",
    "        V_kplus1 = copy.deepcopy(V_k)\n",
    "        delta = epsilon + 1\n",
    "        while delta > epsilon:\n",
    "\n",
    "            delta = 0\n",
    "            for s in states:\n",
    "                v = 0\n",
    "                for n in next_states:\n",
    "\n",
    "                    # Bellman's update rule\n",
    "                    a = int(PI[s])\n",
    "                    v += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "\n",
    "                # Keeps biggest difference seen so far\n",
    "                V_kplus1[s] = v\n",
    "                delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n",
    "\n",
    "            # Updates our current estimate\n",
    "            V_k = V_kplus1\n",
    "            k += 1\n",
    "        all_k.append(k)\n",
    "\n",
    "        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n",
    "        Q = {0: {0: 0,   # state0, action0\n",
    "                 1: 0},  # state0, action1\n",
    "             1: {2: 0}}  # state1, action2\n",
    "        \n",
    "        policy_stable = True\n",
    "        old_PI = copy.deepcopy(PI)\n",
    "        \n",
    "        for s in states: \n",
    "            for a in actions[s]:\n",
    "                for n in next_states:\n",
    "                    \n",
    "                    # Policy Improvement rule\n",
    "                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "                    \n",
    "            PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n",
    "                    \n",
    "            if old_PI[s] != PI[s]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    return V_k, all_k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    \n",
    "    return V, PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modified_policy_iteration():\n",
    "    \n",
    "    return V, PI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZATION\n",
      "\n",
      "V =  [ 0.  0.]\n",
      "\n",
      "PI =  [1 2]\n",
      "\n",
      "\n",
      "RESULTS\n",
      "Policy found in 2 iterations, where each policy evaluation lasted for k = [2, 2]\n",
      "\n",
      "V =  [ 7.622    -3.709875]\n",
      "\n",
      "PI =  [0 2]\n"
     ]
    }
   ],
   "source": [
    "P, R, states, actions, next_states, gamma = create_2statesworld()\n",
    "V_k, all_k, PI = policy_iteration(P, R, states, actions, next_states, gamma)\n",
    "\n",
    "print(\"\\n\\nRESULTS\")\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(all_k), all_k))\n",
    "print(\"\\nV = \", V_k)\n",
    "print(\"\\nPI = \", PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Blablabla blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld MDP\n",
    "\n",
    "(Image taken from Sutton's slides on DP)\n",
    "![grid world](gridworld.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "def create_gridworld(world_size, terminal_states):\n",
    "    \"\"\"\n",
    "    world_size: height and width of the squared-shape gridworld\n",
    "    return\n",
    "        actions: list of str, possible actions\n",
    "        states: list of coordinate tuples representing all non-terminal states\n",
    "        nextState: list of list of dict, index 3 times to return the next state coordinate tuple\n",
    "    \"\"\"\n",
    "\n",
    "    # left, up, right, down\n",
    "    actions = ['L', 'U', 'R', 'D']\n",
    "\n",
    "    # Next\n",
    "    nextState = []\n",
    "    for i in range(0, world_size):\n",
    "        nextState.append([])\n",
    "        for j in range(0, world_size):\n",
    "            # Creates a dictionnary that\n",
    "            next = dict()\n",
    "            if i == 0:\n",
    "                next['U'] = (i, j)\n",
    "            else:\n",
    "                next['U'] = (i - 1, j)\n",
    "\n",
    "            if i == world_size - 1:\n",
    "                next['D'] = (i, j)\n",
    "            else:\n",
    "                next['D'] = (i + 1, j)\n",
    "\n",
    "            if j == 0:\n",
    "                next['L'] = (i, j)\n",
    "            else:\n",
    "                next['L'] = (i, j - 1)\n",
    "\n",
    "            if j == world_size - 1:\n",
    "                next['R'] = (i, j)\n",
    "            else:\n",
    "                next['R'] = (i, j + 1)\n",
    "\n",
    "            nextState[i].append(next)\n",
    "            \n",
    "    states = []\n",
    "    for i in range(0, world_size):\n",
    "        for j in range(0, world_size):\n",
    "            if (i,j) in terminal_states:\n",
    "                continue\n",
    "            else:\n",
    "                states.append((i, j))\n",
    "                \n",
    "    return actions, states, nextState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4, modified=False):\n",
    "\n",
    "    # The reward is always -1\n",
    "    R = -1\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = copy.deepcopy(V_init)\n",
    "    PI = copy.deepcopy(PI_init)\n",
    "    policy_stable = False\n",
    "    all_k = []\n",
    "    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n",
    "\n",
    "    while not policy_stable:\n",
    "        \n",
    "        # 2. POLICY EVALUATION (iterates until V_k converges)\n",
    "        k = 0\n",
    "        V_kplus1 = copy.deepcopy(V_k)\n",
    "        delta = epsilon + 1\n",
    "        \n",
    "        while delta > epsilon and (k < 5 or not modified):\n",
    "\n",
    "            delta = 0\n",
    "            for i, j in states:\n",
    "                \n",
    "                # Here the next state is fully defined by the policy (there is no uncertainty on the transition)\n",
    "                a = idx_to_a[PI[i,j]]\n",
    "                newPosition = nextState[i][j][a]\n",
    "                P = 1.\n",
    "\n",
    "                # Bellman's update rule\n",
    "                V_kplus1[i, j] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n",
    "\n",
    "                # Keeps biggest difference seen so far\n",
    "                delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n",
    "\n",
    "            # Updates our current estimate\n",
    "            V_k = copy.deepcopy(V_kplus1)\n",
    "            k += 1\n",
    "        all_k.append(k)\n",
    "\n",
    "        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n",
    "        Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n",
    "        \n",
    "        policy_stable = True\n",
    "        old_PI = copy.deepcopy(PI)\n",
    "        \n",
    "        for i, j in states:\n",
    "            for a_idx in range(4): # actions\n",
    "                    \n",
    "                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n",
    "                a = idx_to_a[a_idx]\n",
    "                newPosition = nextState[i][j][a]\n",
    "                P = 1.\n",
    "\n",
    "                # Policy Improvement rule\n",
    "                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n",
    "                    \n",
    "            PI[i,j] = np.argmax(Q[i,j,:])\n",
    "                    \n",
    "            if old_PI[i,j] != PI[i,j]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    return V_k, all_k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4):\n",
    "\n",
    "    # The reward is always -1\n",
    "    R = -1\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = copy.deepcopy(V_init)\n",
    "    PI = copy.deepcopy(PI_init)\n",
    "    policy_stable = False\n",
    "    all_k = []\n",
    "    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n",
    "        \n",
    "    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n",
    "    k = 0\n",
    "    V_kplus1 = copy.deepcopy(V_k)\n",
    "    delta = epsilon + 1\n",
    "    Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n",
    "    while delta > epsilon:\n",
    "\n",
    "        # Only one sweep of evaluation before taking the max\n",
    "        delta = 0\n",
    "        for i, j in states:\n",
    "            # Now evaluates the value function for each state for every possible action (not just with respect to current policy)\n",
    "            for a_idx in range(4): # actions\n",
    "\n",
    "                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n",
    "                a = idx_to_a[a_idx]\n",
    "                newPosition = nextState[i][j][a]\n",
    "                P = 1.\n",
    "\n",
    "                # Update rule\n",
    "                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n",
    "\n",
    "            # This step replaces the poilicy improvement step\n",
    "            V_kplus1[i,j] = np.max(Q[i,j,:])\n",
    "\n",
    "            # Keeps biggest difference seen so far\n",
    "            delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n",
    "\n",
    "        # Updates our current estimate\n",
    "        V_k = copy.deepcopy(V_kplus1)\n",
    "        k += 1\n",
    "        \n",
    "    # Updates the policy to be greedy with respect to the value function\n",
    "    for i, j in states:\n",
    "        PI[i,j] = np.argmax(Q[i,j,:])\n",
    "    \n",
    "    return V_k, k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_policy(policy, terminal_states):\n",
    "    \n",
    "    #idx_to_symbol = {0:'\\u25C0', 1:'\\u25BC', 2:'\\u25B6', 3:'\\u25BC'}\n",
    "    idx_to_symbol = {0:'\\u2190', 1:'\\u2191', 2:'\\u2192', 3:'\\u2193'}\n",
    "    \n",
    "    border_str = \"\\u00B7 \"\n",
    "    for i in range(policy.shape[0]):\n",
    "        border_str += \"\\u2015 \"\n",
    "    border_str += \"\\u00B7 \"\n",
    "    #print(border_str)\n",
    "    \n",
    "    for i in range(policy.shape[0]):\n",
    "        \n",
    "        string = \"\"\n",
    "        #string = \"| \"\n",
    "        for j in range(policy.shape[1]):\n",
    "            \n",
    "            if (i,j) in terminal_states:\n",
    "                string += '\\u25A0 '\n",
    "            else:\n",
    "                string += idx_to_symbol[policy[i, j]]+\" \"\n",
    "        \n",
    "        #string += \"|\"\n",
    "        print(string)\n",
    "    \n",
    "    #print(border_str)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZATION\n",
      "Initial value function V is filled with zeros whereas initial policy is random\n",
      "\n",
      "V = \n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "PI = \n",
      "■ ← ↑ ↓ → ← → ↓ ↓ \n",
      "← ↓ ↓ ↓ ← ↑ ↓ ← ↓ \n",
      "↑ → ← ↑ ← ■ ↓ ↑ ↓ \n",
      "↓ → ↓ ↓ ↑ ↑ ← → ← \n",
      "↓ ← ← → → ↑ ← ↓ ↑ \n",
      "↑ ↓ ← ← ← ↓ ↓ ↑ ← \n",
      "↑ ↑ ← ← ↓ ↑ ↓ → → \n",
      "↑ ← ↑ → ↑ → ■ ↓ → \n",
      "→ ↑ → ← ← ↑ ← ↑ ■ \n",
      "\n",
      "\n",
      "RESULTS FOR POLICY ITERATION -------------\n",
      "Policy found in 7 iterations, where each policy evaluation lasted for k = [918, 6, 4, 4, 3, 2, 1]\n",
      "\n",
      "V = \n",
      " [[ 0. -1. -2. -3. -3. -2. -3. -4. -5.]\n",
      " [-1. -2. -3. -3. -2. -1. -2. -3. -4.]\n",
      " [-2. -3. -3. -2. -1.  0. -1. -2. -3.]\n",
      " [-3. -4. -4. -3. -2. -1. -2. -3. -4.]\n",
      " [-4. -5. -5. -4. -3. -2. -3. -4. -4.]\n",
      " [-5. -6. -6. -5. -4. -3. -2. -3. -3.]\n",
      " [-6. -6. -5. -4. -3. -2. -1. -2. -2.]\n",
      " [-6. -5. -4. -3. -2. -1.  0. -1. -1.]\n",
      " [-7. -6. -5. -4. -3. -2. -1. -1.  0.]]\n",
      "\n",
      "PI = \n",
      "■ ← ← ← → ↓ ← ← ← \n",
      "↑ ← ← → → ↓ ← ← ← \n",
      "↑ ← → → → ■ ← ← ← \n",
      "↑ ← ↑ ↑ ↑ ↑ ← ← ← \n",
      "↑ ← ↑ ↑ ↑ ↑ ← ← ↓ \n",
      "↑ ← ↑ ↑ ↑ ↑ ↓ ← ↓ \n",
      "↑ → → → → → ↓ ← ↓ \n",
      "→ → → → → → ■ ← ↓ \n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ → ■ \n",
      "\n",
      "\n",
      "RESULTS FOR VALUE ITERATION -------------\n",
      "Policy found in 8 iterations\n",
      "\n",
      "V = \n",
      " [[ 0. -1. -2. -3. -3. -2. -3. -4. -5.]\n",
      " [-1. -2. -3. -3. -2. -1. -2. -3. -4.]\n",
      " [-2. -3. -3. -2. -1.  0. -1. -2. -3.]\n",
      " [-3. -4. -4. -3. -2. -1. -2. -3. -4.]\n",
      " [-4. -5. -5. -4. -3. -2. -3. -4. -4.]\n",
      " [-5. -6. -6. -5. -4. -3. -2. -3. -3.]\n",
      " [-6. -6. -5. -4. -3. -2. -1. -2. -2.]\n",
      " [-6. -5. -4. -3. -2. -1.  0. -1. -1.]\n",
      " [-7. -6. -5. -4. -3. -2. -1. -1.  0.]]\n",
      "\n",
      "PI = \n",
      "■ ← ← ← → ↓ ← ← ← \n",
      "↑ ← ← → → ↓ ← ← ← \n",
      "↑ ← → → → ■ ← ← ← \n",
      "↑ ← ↑ ↑ ↑ ↑ ← ← ← \n",
      "↑ ← ↑ ↑ ↑ ↑ ← ← ↓ \n",
      "↑ ← ↑ ↑ ↑ ↑ ↓ ← ↓ \n",
      "↑ → → → → → ↓ ← ↓ \n",
      "→ → → → → → ■ ← ↓ \n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ → ■ \n",
      "\n",
      "\n",
      "RESULTS FOR MODIFIED POLICY ITERATION -------------\n",
      "Policy found in 7 iterations, where each policy evaluation lasted for k = [5, 5, 5, 5, 5, 2, 1]\n",
      "\n",
      "V = \n",
      " [[ 0. -1. -2. -3. -3. -2. -3. -4. -5.]\n",
      " [-1. -2. -3. -3. -2. -1. -2. -3. -4.]\n",
      " [-2. -3. -3. -2. -1.  0. -1. -2. -3.]\n",
      " [-3. -4. -4. -3. -2. -1. -2. -3. -4.]\n",
      " [-4. -5. -5. -4. -3. -2. -3. -4. -4.]\n",
      " [-5. -6. -6. -5. -4. -3. -2. -3. -3.]\n",
      " [-6. -6. -5. -4. -3. -2. -1. -2. -2.]\n",
      " [-6. -5. -4. -3. -2. -1.  0. -1. -1.]\n",
      " [-7. -6. -5. -4. -3. -2. -1. -1.  0.]]\n",
      "\n",
      "PI = \n",
      "■ ← ← ← → ↓ ← ← ← \n",
      "↑ ← ← → → ↓ ← ← ← \n",
      "↑ ← → → → ■ ← ← ← \n",
      "↑ ← ↑ ↑ ↑ ↑ ← ← ← \n",
      "↑ ← ↑ ↑ ↑ ↑ ← ← ↓ \n",
      "↑ ← ↑ ↑ ↑ ↑ ↓ ← ↓ \n",
      "↑ → → → → → ↓ ← ↓ \n",
      "→ → → → → → ■ ← ↓ \n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ → ■ \n"
     ]
    }
   ],
   "source": [
    "# Hyper-params\n",
    "gamma = 0.99\n",
    "epsilon = 1e-4\n",
    "\n",
    "# The GRIDWORLD\n",
    "world_size = 9\n",
    "terminal_states = [(0,0), (world_size-1, world_size-1), (world_size-2, world_size-3), (2, world_size-int(world_size/2))]\n",
    "actions, states, nextState = create_gridworld(world_size, terminal_states)\n",
    "\n",
    "# Initializations\n",
    "V_init = np.zeros((world_size, world_size), dtype=np.float)    # V(s) ... our value function estimate for PI\n",
    "PI_init = np.random.randint(low=0, high=4, size=(world_size, world_size), dtype=np.int)     # PI(s) ... our greedy policy\n",
    "\n",
    "print(\"INITIALIZATION\")\n",
    "print(\"Initial value function V is filled with zeros whereas initial policy is random\")\n",
    "print(\"\\nV = \\n\", np.round(V_init))\n",
    "print(\"\\nPI = \")\n",
    "print_policy(PI_init, terminal_states)\n",
    "\n",
    "PolIt_results = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR POLICY ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(PolIt_results[1]), PolIt_results[1]))\n",
    "print(\"\\nV = \\n\", np.round(PolIt_results[0]))\n",
    "print(\"\\nPI = \")\n",
    "print_policy(PolIt_results[2], terminal_states)\n",
    "\n",
    "ValIt_results = value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR VALUE ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations\".format(ValIt_results[1]))\n",
    "print(\"\\nV = \\n\", np.round(ValIt_results[0]))\n",
    "print(\"\\nPI = \")\n",
    "print_policy(ValIt_results[2], terminal_states)\n",
    "\n",
    "M_PolIt_results = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon, modified=True)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR MODIFIED POLICY ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(M_PolIt_results[1]), M_PolIt_results[1]))\n",
    "print(\"\\nV = \\n\", np.round(M_PolIt_results[0]))\n",
    "print(\"\\nPI = \")\n",
    "print_policy(M_PolIt_results[2], terminal_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Bl abla bla bla bla"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
