{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Bla bla bla MDP bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-states MDP\n",
    "\n",
    "* $P(s_0 | s_0, a_0) = 0.5,  r = 5$\n",
    "* $P(s_1 | s_0, a_0) = 0.5,  r = 5$\n",
    "* $P(s_0 | s_0, a_1) = 0$\n",
    "* $P(s_1 | s_0, a_1) = 1,  r = 10$\n",
    "* $P(s_1 | s_0, a_2) = 0$\n",
    "* $P(s_1 | s_1, a_2) = 1,  r = -1$\n",
    "\n",
    "\n",
    "* $\\gamma = 0.95$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_2statesworld():\n",
    "\n",
    "    P = np.zeros((2,2,3)) # P(s'|s,a) ... our model of the environment\n",
    "    P[0,0,0] = 0.5\n",
    "    P[1,0,0] = 0.5\n",
    "    P[1,0,1] = 1.\n",
    "    P[1,1,2] = 1.\n",
    "\n",
    "    R = np.zeros((2,3))  # R(s,a) ... the reward funciton \n",
    "    R[0,0] = 5\n",
    "    R[0,1] = 10\n",
    "    R[1,2] = -1\n",
    "\n",
    "    states = [0, 1]\n",
    "    actions = [[0, 1], [2]]\n",
    "    next_states = [0, 1]\n",
    "    gamma = 0.95\n",
    "    \n",
    "    return P, R, states, actions, next_states, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(P, R, states, actions, next_states, gamma, epsilon=1e-4):\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = np.zeros((2,), dtype=np.float) # V(s) ... our value function estimate for PI\n",
    "    PI = np.zeros((2), dtype=np.int)     # PI(s) ... our greedy policy\n",
    "    policy_stable = False\n",
    "    all_k = []\n",
    "    \n",
    "    while not policy_stable:\n",
    "        \n",
    "        # 2. POLICY EVALUATION (iterates until V_k converges) \n",
    "        k = 0\n",
    "        V_kplus1 = copy.deepcopy(V_k)\n",
    "        delta = epsilon + 1\n",
    "        while delta > epsilon:\n",
    "\n",
    "            delta = 0\n",
    "            for s in states:\n",
    "                for n in next_states:\n",
    "\n",
    "                    # Bellman's update rule\n",
    "                    a = int(PI[s])\n",
    "                    V_kplus1[s] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "\n",
    "                    # Keeps biggest difference seen so far\n",
    "                    delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n",
    "\n",
    "            # Updates our current estimate\n",
    "            V_k = V_kplus1\n",
    "            k += 1\n",
    "        all_k.append(k)\n",
    "\n",
    "        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n",
    "        Q = {0: {0: 0,   # state0, action0\n",
    "                 1: 0},  # state0, action1\n",
    "             1: {2: 0}}  # state1, action2\n",
    "        \n",
    "        policy_stable = True\n",
    "        old_PI = copy.deepcopy(PI)\n",
    "        \n",
    "        for s in states: \n",
    "            for a in actions[s]:\n",
    "                for n in next_states:\n",
    "                    \n",
    "                    # Policy Improvement rule\n",
    "                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "                    \n",
    "            PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n",
    "                    \n",
    "            if old_PI[s] != PI[s]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    return V_k, all_k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    \n",
    "    return V, PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modified_policy_iteration():\n",
    "    \n",
    "    return V, PI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy found in 2 iterations, where each policy evaluation lasted for k = [2, 2]\n",
      "V =  [ 38.82335938  -2.95      ]\n",
      "PI =  [0 2]\n"
     ]
    }
   ],
   "source": [
    "P, R, states, actions, next_states, gamma = create_2statesworld()\n",
    "V_k, all_k, PI = policy_iteration(P, R, states, actions, next_states, gamma)\n",
    "\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(all_k), all_k))\n",
    "print(\"V = \", V_k)\n",
    "print(\"PI = \", PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Blablabla blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld MDP\n",
    "\n",
    "(Image taken from Sutton's slides on DP)\n",
    "![grid world](gridworld.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "def create_gridworld(world_size):\n",
    "    \"\"\"\n",
    "    world_size: height and width of the squared-shape gridworld\n",
    "    return\n",
    "        actions: list of str, possible actions\n",
    "        states: list of coordinate tuples representing all non-terminal states\n",
    "        nextState: list of list of dict, index 3 times to return the next state coordinate tuple\n",
    "    \"\"\"\n",
    "\n",
    "    # left, up, right, down\n",
    "    actions = ['L', 'U', 'R', 'D']\n",
    "\n",
    "    # Next\n",
    "    nextState = []\n",
    "    for i in range(0, WORLD_SIZE):\n",
    "        nextState.append([])\n",
    "        for j in range(0, WORLD_SIZE):\n",
    "            # Creates a dictionnary that\n",
    "            next = dict()\n",
    "            if i == 0:\n",
    "                next['U'] = (i, j)\n",
    "            else:\n",
    "                next['U'] = (i - 1, j)\n",
    "\n",
    "            if i == WORLD_SIZE - 1:\n",
    "                next['D'] = (i, j)\n",
    "            else:\n",
    "                next['D'] = (i + 1, j)\n",
    "\n",
    "            if j == 0:\n",
    "                next['L'] = (i, j)\n",
    "            else:\n",
    "                next['L'] = (i, j - 1)\n",
    "\n",
    "            if j == WORLD_SIZE - 1:\n",
    "                next['R'] = (i, j)\n",
    "            else:\n",
    "                next['R'] = (i, j + 1)\n",
    "\n",
    "            nextState[i].append(next)\n",
    "\n",
    "    states = []\n",
    "    for i in range(0, WORLD_SIZE):\n",
    "        for j in range(0, WORLD_SIZE):\n",
    "            if (i == 0 and j == 0) or (i == WORLD_SIZE - 1 and j == WORLD_SIZE - 1):\n",
    "                continue\n",
    "            else:\n",
    "                states.append((i, j))\n",
    "                \n",
    "    return actions, states, nextState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(world_size, states, actions, nextState, gamma, epsilon=1e-4):\n",
    "\n",
    "    # The reward is always -1\n",
    "    R = -1\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = np.zeros((world_size, world_size), dtype=np.float)  # V(s) ... our value function estimate for PI\n",
    "    PI = np.zeros((world_size, world_size), dtype=np.int)     # PI(s) ... our greedy policy\n",
    "    policy_stable = False\n",
    "    all_k = []\n",
    "    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n",
    "    \n",
    "    while not policy_stable:\n",
    "        \n",
    "        # 2. POLICY EVALUATION (iterates until V_k converges) \n",
    "        k = 0\n",
    "        V_kplus1 = copy.deepcopy(V_k)\n",
    "        delta = epsilon + 1\n",
    "        while delta > epsilon:\n",
    "\n",
    "            delta = 0\n",
    "            for i, j in states:\n",
    "\n",
    "                # Here the next state is fully defined by the policy (there is no uncertainty on the transition)\n",
    "                a = idx_to_a[PI[i,j]]\n",
    "                newPosition = nextState[i][j][a]\n",
    "                P = 1.\n",
    "\n",
    "                # Bellman's update rule\n",
    "                V_kplus1[i, j] += P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n",
    "\n",
    "                # Keeps biggest difference seen so far\n",
    "                delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n",
    "\n",
    "            # Updates our current estimate\n",
    "            V_k = V_kplus1\n",
    "            k += 1\n",
    "        all_k.append(k)\n",
    "\n",
    "        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n",
    "        Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n",
    "        \n",
    "        policy_stable = True\n",
    "        old_PI = copy.deepcopy(PI)\n",
    "        \n",
    "        for i, j in states:\n",
    "            for idx in range(4): # actions\n",
    "                    \n",
    "                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n",
    "                a = idx_to_a[idx]\n",
    "                newPosition = nextState[i][j][a]\n",
    "                P = 1.\n",
    "\n",
    "                # Policy Improvement rule\n",
    "                Q[i,j,idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n",
    "                    \n",
    "            PI[i,j] = np.argmax(Q[i,j,:])\n",
    "                    \n",
    "            if old_PI[i,j] != PI[i,j]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    return V_k, all_k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    \n",
    "    return V, PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modified_policy_iteration():\n",
    "    \n",
    "    return V, PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    \n",
    "    #idx_to_symbol = {0:'\\u25C0', 1:'\\u25BC', 2:'\\u25B6', 3:'\\u25BC'}\n",
    "    idx_to_symbol = {0:'\\u2190', 1:'\\u2191', 2:'\\u2192', 3:'\\u2193'}\n",
    "    \n",
    "    for i in range(policy.shape[0]):\n",
    "        \n",
    "        string = \"|\"\n",
    "        for j in range(policy.shape[1]):\n",
    "            \n",
    "            if (i == 0 and j == 0) or (i == policy.shape[0]-1 and j == policy.shape[1]-1):\n",
    "                string += '\\u25A0'\n",
    "            else:\n",
    "                string += idx_to_symbol[policy[i, j]]\n",
    "        \n",
    "        string += \"|\"\n",
    "        print(string)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy found in 4 iterations, where each policy evaluation lasted for k = [2, 2, 2, 2]\n",
      "V = \n",
      " [[   0.   -8.  -38. -118.]\n",
      " [  -9.  -39. -119.  -86.]\n",
      " [ -45. -122.  -77.  -14.]\n",
      " [-127.  -65.  -13.    0.]]\n",
      "PI = \n",
      "|■←←←|\n",
      "|↑↑↑↓|\n",
      "|↑↑↓↓|\n",
      "|↑→→■|\n"
     ]
    }
   ],
   "source": [
    "actions, states, nextState = create_gridworld(world_size=4)\n",
    "V_k, all_k, PI = policy_iteration(4, states, actions, nextState, gamma, epsilon=1e-4)\n",
    "\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(all_k), all_k))\n",
    "print(\"V = \\n\", np.round(V_k))\n",
    "print(\"PI = \")\n",
    "print_policy(PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Bl abla bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_policy(PI):\n",
    "    \n",
    "    for i in range(PI.shape[0]):\n",
    "        print(\"|O<<<|\")\n",
    "        print(\"|^<^v|\")\n",
    "        print(\"|^>vv|\")\n",
    "        print(\"|^>>O|\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
