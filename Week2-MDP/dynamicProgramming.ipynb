{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 2 : MDPs AND DYNAMIC PROGRAMMING\n",
    "\n",
    "**Due date : 05/02/2018**\n",
    "\n",
    "**By : Julien Roy and David Kanaa**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1 - Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 : \n",
    "\n",
    "Bla bla bla \n",
    "\n",
    "$$Maths + maths = Maths$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : \n",
    "\n",
    "\n",
    "Bla bla bla \n",
    "\n",
    "$$Maths + maths = Maths$$\n",
    "$$ Maths + 2 * 3 = 2x + 5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2 - PROGRAMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Bla bla bla MDP bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-states MDP\n",
    "\n",
    "* $P(s_0 | s_0, a_0) = 0.5,  r = 5$\n",
    "* $P(s_1 | s_0, a_0) = 0.5,  r = 5$\n",
    "* $P(s_0 | s_0, a_1) = 0$\n",
    "* $P(s_1 | s_0, a_1) = 1,  r = 10$\n",
    "* $P(s_1 | s_0, a_2) = 0$\n",
    "* $P(s_1 | s_1, a_2) = 1,  r = -1$\n",
    "\n",
    "\n",
    "* $\\gamma = 0.95$\n",
    "\n",
    "![2-states world](2-states_world.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_2statesworld():\n",
    "\n",
    "    P = np.zeros((2,2,3)) # P(s'|s,a) ... our model of the environment\n",
    "    P[0,0,0] = 0.5\n",
    "    P[1,0,0] = 0.5\n",
    "    P[1,0,1] = 1.\n",
    "    P[1,1,2] = 1.\n",
    "\n",
    "    R = np.zeros((2,3))  # R(s,a) ... the reward funciton \n",
    "    R[0,0] = 5\n",
    "    R[0,1] = 10\n",
    "    R[1,2] = -1\n",
    "\n",
    "    states = [0, 1]\n",
    "    actions = [[0, 1], [2]]\n",
    "    next_states = [0, 1]\n",
    "    gamma = 0.95\n",
    "    \n",
    "    return P, R, states, actions, next_states, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, epsilon=1e-4, modified=False):\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = copy.deepcopy(V_init)  # V(s) ... our value function estimate for PI\n",
    "    PI = copy.deepcopy(PI_init)  # PI(s) ... our greedy policy\n",
    "    policy_stable = False\n",
    "    all_k = []\n",
    "    \n",
    "    while not policy_stable:\n",
    "        \n",
    "        # 2. POLICY EVALUATION (iterates until V_k converges) \n",
    "        k = 0\n",
    "        V_kplus1 = copy.deepcopy(V_k)\n",
    "        delta = epsilon + 1\n",
    "        while delta > epsilon and (k < 5 or not modified):\n",
    "\n",
    "            delta = 0\n",
    "            for s in states:\n",
    "                v = 0\n",
    "                for n in next_states:\n",
    "\n",
    "                    # Bellman's update rule\n",
    "                    a = int(PI[s])\n",
    "                    v += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "\n",
    "                # Keeps biggest difference seen so far\n",
    "                V_kplus1[s] = v\n",
    "                delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n",
    "\n",
    "            # Updates our current estimate\n",
    "            V_k = copy.deepcopy(V_kplus1)\n",
    "            k += 1\n",
    "        all_k.append(k)\n",
    "\n",
    "        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n",
    "        Q = {0: {0: 0,   # state0, action0\n",
    "                 1: 0},  # state0, action1\n",
    "             1: {2: 0}}  # state1, action2\n",
    "        \n",
    "        policy_stable = True\n",
    "        old_PI = copy.deepcopy(PI)\n",
    "        \n",
    "        for s in states: \n",
    "            for a in actions[s]:\n",
    "                for n in next_states:\n",
    "                    \n",
    "                    # Policy Improvement rule\n",
    "                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "                    \n",
    "            PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n",
    "                    \n",
    "            if old_PI[s] != PI[s]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    return V_k, all_k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, epsilon=1e-4):\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = copy.deepcopy(V_init)  # V(s) ... our value function estimate for PI\n",
    "    PI = copy.deepcopy(PI_init)  # PI(s) ... our greedy policy\n",
    "        \n",
    "    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n",
    "    k = 0\n",
    "    V_kplus1 = copy.deepcopy(V_k)\n",
    "    delta = epsilon + 1\n",
    "    \n",
    "    while delta > epsilon:\n",
    "\n",
    "        delta = 0\n",
    "        \n",
    "        Q = {0: {0: 0,   # state0, action0\n",
    "                 1: 0},  # state0, action1\n",
    "             1: {2: 0}}  # state1, action2\n",
    "        for s in states:\n",
    "            v = 0\n",
    "            for a in actions[s]:\n",
    "                for n in next_states:\n",
    "                \n",
    "                    # Bellman's optimality update rule\n",
    "                    Q[s][a] += P[n,s,a] * (R[s,a] + gamma * V_k[n])\n",
    "\n",
    "            # This step replaces the poilicy improvement step (gets the maximal value)\n",
    "            V_kplus1[s] = max(Q[s].items(), key=operator.itemgetter(1))[1]\n",
    "            \n",
    "            # Keeps biggest difference seen so far\n",
    "            delta = np.max([delta, np.abs(V_kplus1[s] - V_k[s])])\n",
    "\n",
    "        # Updates our current estimate\n",
    "        V_k = copy.deepcopy(V_kplus1)\n",
    "        k += 1\n",
    "    \n",
    "    # Updates the policy to be greedy with respect to the value function\n",
    "    for s in states:\n",
    "        PI[s] = max(Q[s].items(), key=operator.itemgetter(1))[0]\n",
    "    \n",
    "    return V_k, k, PI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZATION\n",
      "Initial value function V is filled with zeros whereas initial policy is random among legal actions for each state\n",
      "\n",
      "V =  [ 0.  0.]\n",
      "\n",
      "PI =  [1 2]\n",
      "\n",
      "\n",
      "RESULTS FOR POLICY ITERATION -------------\n",
      "Policy found in 2 iterations, where each policy evaluation lasted for k = [181, 11]\n",
      "\n",
      "V =  [ -8.57 -20.  ]\n",
      "\n",
      "PI =  [0 2]\n",
      "\n",
      "\n",
      "RESULTS FOR VALUE ITERATION -------------\n",
      "Policy found in 181 iterations\n",
      "\n",
      "V = \n",
      " [ -8.57 -20.  ]\n",
      "\n",
      "PI =  [0 2]\n",
      "\n",
      "\n",
      "RESULTS FOR MODIFIED POLICY ITERATION -------------\n",
      "Policy found in 2 iterations, where each policy evaluation lasted for k = [5, 5]\n",
      "\n",
      "V =  [ 3.39 -8.03]\n",
      "\n",
      "PI =  [0 2]\n"
     ]
    }
   ],
   "source": [
    "# Initializations\n",
    "V_init = np.zeros((2,), dtype=np.float)                            # V(s) ... our value function estimate for PI\n",
    "PI_init = np.array([np.random.uniform(0, 2), 2], dtype=np.int)     # PI(s) ... our greedy policy\n",
    "\n",
    "# The 2-states world\n",
    "P, R, states, actions, next_states, gamma = create_2statesworld()\n",
    "\n",
    "print(\"INITIALIZATION\")\n",
    "print(\"Initial value function V is filled with zeros whereas initial policy is random among legal actions for each state\")\n",
    "print(\"\\nV = \", np.round(V_init))\n",
    "print(\"\\nPI = \", PI_init)\n",
    "\n",
    "\n",
    "PolIt_results = policy_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR POLICY ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(PolIt_results[1]), PolIt_results[1]))\n",
    "print(\"\\nV = \", np.round(PolIt_results[0], 2))\n",
    "print(\"\\nPI = \", PolIt_results[2])\n",
    "\n",
    "ValIt_results = value_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR VALUE ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations\".format(ValIt_results[1]))\n",
    "print(\"\\nV = \\n\", np.round(ValIt_results[0], 2))\n",
    "print(\"\\nPI = \", ValIt_results[2])\n",
    "\n",
    "\n",
    "M_PolIt_results = policy_iteration(V_init, PI_init, P, R, states, actions, next_states, gamma, modified=True)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR MODIFIED POLICY ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(M_PolIt_results[1]), M_PolIt_results[1]))\n",
    "print(\"\\nV = \", np.round(M_PolIt_results[0], 2))\n",
    "print(\"\\nPI = \", M_PolIt_results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Blablabla blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld MDP\n",
    "\n",
    "(Image taken from Sutton's slides on DP)\n",
    "![grid world](gridworld.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "def create_gridworld(world_size, terminal_states):\n",
    "    \"\"\"\n",
    "    world_size: height and width of the squared-shape gridworld\n",
    "    return\n",
    "        actions: list of str, possible actions\n",
    "        states: list of coordinate tuples representing all non-terminal states\n",
    "        nextState: list of list of dict, index 3 times to return the next state coordinate tuple\n",
    "    \"\"\"\n",
    "\n",
    "    # left, up, right, down\n",
    "    actions = ['L', 'U', 'R', 'D']\n",
    "\n",
    "    # Next\n",
    "    nextState = []\n",
    "    for i in range(0, world_size):\n",
    "        nextState.append([])\n",
    "        for j in range(0, world_size):\n",
    "            # Creates a dictionnary that\n",
    "            next = dict()\n",
    "            if i == 0:\n",
    "                next['U'] = (i, j)\n",
    "            else:\n",
    "                next['U'] = (i - 1, j)\n",
    "\n",
    "            if i == world_size - 1:\n",
    "                next['D'] = (i, j)\n",
    "            else:\n",
    "                next['D'] = (i + 1, j)\n",
    "\n",
    "            if j == 0:\n",
    "                next['L'] = (i, j)\n",
    "            else:\n",
    "                next['L'] = (i, j - 1)\n",
    "\n",
    "            if j == world_size - 1:\n",
    "                next['R'] = (i, j)\n",
    "            else:\n",
    "                next['R'] = (i, j + 1)\n",
    "\n",
    "            nextState[i].append(next)\n",
    "            \n",
    "    states = []\n",
    "    for i in range(0, world_size):\n",
    "        for j in range(0, world_size):\n",
    "            if (i,j) in terminal_states:\n",
    "                continue\n",
    "            else:\n",
    "                states.append((i, j))\n",
    "                \n",
    "    return actions, states, nextState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4, modified=False):\n",
    "\n",
    "    # The reward is always -1\n",
    "    R = -1\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = copy.deepcopy(V_init)\n",
    "    PI = copy.deepcopy(PI_init)\n",
    "    policy_stable = False\n",
    "    all_k = []\n",
    "    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n",
    "\n",
    "    while not policy_stable:\n",
    "        \n",
    "        # 2. POLICY EVALUATION (iterates until V_k converges)\n",
    "        k = 0\n",
    "        V_kplus1 = copy.deepcopy(V_k)\n",
    "        delta = epsilon + 1\n",
    "        \n",
    "        while delta > epsilon and (k < 5 or not modified):\n",
    "\n",
    "            delta = 0\n",
    "            for i, j in states:\n",
    "                \n",
    "                # Here the next state is fully defined by the policy (there is no uncertainty on the transition)\n",
    "                a = idx_to_a[PI[i,j]]\n",
    "                newPosition = nextState[i][j][a]\n",
    "                P = 1.\n",
    "\n",
    "                # Bellman's update rule\n",
    "                V_kplus1[i, j] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n",
    "\n",
    "                # Keeps biggest difference seen so far\n",
    "                delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n",
    "\n",
    "            # Updates our current estimate\n",
    "            V_k = copy.deepcopy(V_kplus1)\n",
    "            k += 1\n",
    "        all_k.append(k)\n",
    "\n",
    "        # 3. POLICY IMPROVEMENT (greedy action selection with respect to V_k)\n",
    "        Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n",
    "        \n",
    "        policy_stable = True\n",
    "        old_PI = copy.deepcopy(PI)\n",
    "        \n",
    "        for i, j in states:\n",
    "            for a_idx in range(4): # actions\n",
    "                    \n",
    "                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n",
    "                a = idx_to_a[a_idx]\n",
    "                newPosition = nextState[i][j][a]\n",
    "                P = 1.\n",
    "\n",
    "                # Policy Improvement rule\n",
    "                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n",
    "                    \n",
    "            PI[i,j] = np.argmax(Q[i,j,:])\n",
    "                    \n",
    "            if old_PI[i,j] != PI[i,j]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    return V_k, all_k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon=1e-4):\n",
    "\n",
    "    # The reward is always -1\n",
    "    R = -1\n",
    "    \n",
    "    #1. INITIALIZATION\n",
    "    V_k = copy.deepcopy(V_init)\n",
    "    PI = copy.deepcopy(PI_init)\n",
    "    idx_to_a = {0:'L', 1:'U', 2:'R', 3:'D'}\n",
    "        \n",
    "    # 2. POLICY EVALUATION (makes only 1 sweep before taking the max over the actions)\n",
    "    k = 0\n",
    "    V_kplus1 = copy.deepcopy(V_k)\n",
    "    delta = epsilon + 1\n",
    "    Q = np.zeros((world_size, world_size, 4), dtype=np.float)\n",
    "    while delta > epsilon:\n",
    "\n",
    "        # Only one sweep of evaluation before taking the max\n",
    "        delta = 0\n",
    "        for i, j in states:\n",
    "            # Now evaluates the value function for each state for every possible action (not just with respect to current policy)\n",
    "            for a_idx in range(4): # actions\n",
    "\n",
    "                # Again the next state is fully defined by the chosen action (there is no uncertainty on the transition)\n",
    "                a = idx_to_a[a_idx]\n",
    "                newPosition = nextState[i][j][a]\n",
    "                P = 1.\n",
    "\n",
    "                # Update rule\n",
    "                Q[i,j,a_idx] = P * (R + gamma * V_k[newPosition[0], newPosition[1]])\n",
    "\n",
    "            # This step replaces the poilicy improvement step\n",
    "            V_kplus1[i,j] = np.max(Q[i,j,:])\n",
    "\n",
    "            # Keeps biggest difference seen so far\n",
    "            delta = np.max([delta, np.abs(V_kplus1[i,j] - V_k[i,j])])\n",
    "\n",
    "        # Updates our current estimate\n",
    "        V_k = copy.deepcopy(V_kplus1)\n",
    "        k += 1\n",
    "        \n",
    "    # Updates the policy to be greedy with respect to the value function\n",
    "    for i, j in states:\n",
    "        PI[i,j] = np.argmax(Q[i,j,:])\n",
    "    \n",
    "    return V_k, k, PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_policy(policy, terminal_states):\n",
    "    \n",
    "    #idx_to_symbol = {0:'\\u25C0', 1:'\\u25BC', 2:'\\u25B6', 3:'\\u25BC'}\n",
    "    idx_to_symbol = {0:'\\u2190', 1:'\\u2191', 2:'\\u2192', 3:'\\u2193'}\n",
    "    \n",
    "    border_str = \"\\u00B7 \"\n",
    "    for i in range(policy.shape[0]):\n",
    "        border_str += \"\\u2015 \"\n",
    "    border_str += \"\\u00B7 \"\n",
    "    #print(border_str)\n",
    "    \n",
    "    for i in range(policy.shape[0]):\n",
    "        \n",
    "        string = \"\"\n",
    "        #string = \"| \"\n",
    "        for j in range(policy.shape[1]):\n",
    "            \n",
    "            if (i,j) in terminal_states:\n",
    "                string += '\\u25A0 '\n",
    "            else:\n",
    "                string += idx_to_symbol[policy[i, j]]+\" \"\n",
    "        \n",
    "        #string += \"|\"\n",
    "        print(string)\n",
    "    \n",
    "    #print(border_str)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZATION\n",
      "Initial value function V is filled with zeros whereas initial policy is random\n",
      "\n",
      "V = \n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      "PI = \n",
      "■ ↓ ↑ ↓ ↓ \n",
      "↑ → ↑ → ↑ \n",
      "↑ ↑ ↓ ■ ← \n",
      "← → ■ → → \n",
      "← ← ↓ ↑ ■ \n",
      "\n",
      "\n",
      "RESULTS FOR POLICY ITERATION -------------\n",
      "Policy found in 4 iterations, where each policy evaluation lasted for k = [918, 5, 2, 1]\n",
      "\n",
      "V = \n",
      " [[ 0. -1. -2. -2. -3.]\n",
      " [-1. -2. -2. -1. -2.]\n",
      " [-2. -2. -1.  0. -1.]\n",
      " [-2. -1.  0. -1. -1.]\n",
      " [-3. -2. -1. -1.  0.]]\n",
      "\n",
      "PI = \n",
      "■ ← ← ↓ ← \n",
      "↑ ← → ↓ ← \n",
      "↑ → → ■ ← \n",
      "→ → ■ ← ↓ \n",
      "↑ ↑ ↑ → ■ \n",
      "\n",
      "\n",
      "RESULTS FOR VALUE ITERATION -------------\n",
      "Policy found in 4 iterations\n",
      "\n",
      "V = \n",
      " [[ 0. -1. -2. -2. -3.]\n",
      " [-1. -2. -2. -1. -2.]\n",
      " [-2. -2. -1.  0. -1.]\n",
      " [-2. -1.  0. -1. -1.]\n",
      " [-3. -2. -1. -1.  0.]]\n",
      "\n",
      "PI = \n",
      "■ ← ← ↓ ← \n",
      "↑ ← → ↓ ← \n",
      "↑ → → ■ ← \n",
      "→ → ■ ← ↓ \n",
      "↑ ↑ ↑ → ■ \n",
      "\n",
      "\n",
      "RESULTS FOR MODIFIED POLICY ITERATION -------------\n",
      "Policy found in 4 iterations, where each policy evaluation lasted for k = [5, 5, 2, 1]\n",
      "\n",
      "V = \n",
      " [[ 0. -1. -2. -2. -3.]\n",
      " [-1. -2. -2. -1. -2.]\n",
      " [-2. -2. -1.  0. -1.]\n",
      " [-2. -1.  0. -1. -1.]\n",
      " [-3. -2. -1. -1.  0.]]\n",
      "\n",
      "PI = \n",
      "■ ← ← ↓ ← \n",
      "↑ ← → ↓ ← \n",
      "↑ → → ■ ← \n",
      "→ → ■ ← ↓ \n",
      "↑ ↑ ↑ → ■ \n",
      "\n",
      "\n",
      "EFFECT OF GAMMA ON CONVERGENCE SPEED\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAEWCAYAAADSL2tlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8XGXZ//HP1Sxt0i0tXdOFtnSlQIsUkL2yr0X4KY/I\n44IooiK4IYgKCvKACy6PwoOgCG5FFMW0lF0pKiCUmnQvtKXQNt3bdEuzX78/zj3NJGQ5TTuZSfJ9\nv17zmjn7NXOSzJX7vs91zN0RERERkczRLd0BiIiIiEhDStBEREREMowSNBEREZEMowRNREREJMMo\nQRMRERHJMErQRERERDKMEjSRVphZtpk9ZGZlZuZm9oMw/zozKw3z5qU7zgQzG2FmfzezvSG2C9Md\nk0hHZGbfCr9Dj6c7Ful6lKBJl2dmq8Mf4caPqWGVS4GPATXAT4EXzWwo8CNgCPAg8PuDGMf0A9zV\n14DpwArgJ8CqA9yfiIi0s+x0ByCSQWYDK5OmN4fn8eF5jrtfB2BmJxP9g7PG3a9qvxBjScT7I3d/\nMK2RSFqYWba716Q7DhE5AO6uhx5d+gGsBhx4fxPLvhWWJT8+3sS8h8L6JwMvANuBUqLWtUOS9jcJ\n+EtYthcoBg5NiqHBcZqJ91TgRaAs7Od3QGFY9kIT+xnVxD76An8AdgILgC+FdcuS1vk9sA6oBHYB\nfwOObOJzuw1YDOwGfgwcDrwetpkJdA/rJz63EqLWx93AEuBo4HZgB1Fr39lJx7g7HKcCKAdeAaa3\ncC5ziFoQlwF7gKXAF4FujWL4Z4ihLLzHK1rYZzZwPbAoxLARuCUsM+BqYGE43grgO0CPsHx6ON5q\n4GZgU3jcEJa/LyxfmHS808K8RWE6H7gr7HsPMJ+kn1XgobD+z4Fngapw3DjneCTwSPgMyoBngCOa\nOMc3Af8Jx58D9Eta571hu83hnL4C5IdlRwBPhPe8GXgMGNnCZ/0Fon+SKoEtRD/PExr9bP8P8FKI\n5e8k/Xy3drwY7/fwEH85MIuoxdyBx9P9d0qPrvdIewB66JHuR9KX0CyiBOPHwI/DsnPDH2wnSiZ+\nDBwH/CnM2xnmfTh8OSSSmZnAU2GdvxF9kQ8JXzoevmTvD89TgVvCvjzs+8fAcU3EelT4Aq4LXzQv\nh20WECUn1wJrw7xnwn76N7Gfh8M6a4FfJR07+cv7X0TJ3z3A3LB8aROf266wv6owvS1sl3ivnwzr\nfzxM14V9v5Y4JlEi9XQipqRj/JkoybgnfPF6+OLt3cy5/F5YpxT4ZVjXga81isGBV8Nn5ESJRZ9m\n9nlHWKcc+E343H8Xln026T08mPSZ/Dwsn550vCXh/SQ+g3Hh5+LtMG9S2OaeMP3VMD0zTL8OPECU\nINYRElXqEzQnSmIeBN7T2jkmSvxWhH09Cfw6nMNNwIBG57g87G9rmL49LD+CKHl2on8aHgCWAwVE\nP+/bwj7/DPwxrLeUkLQ3+pzHJp3f+4j+QViV9D5fCMurwnlYGKZfC8tbPF5r75coEV+R2CfwKNGw\nBiVoeqTlkfYA9NAj3Q+abr3ypOXfIqmVLMybHuatTpqX+GJ9hfpEL/HlNRH4CvXJWbek7bIbxTG9\nhVjvDev8KkznEH1hO6HlKemL7OPN7COLKJF04LQw74u8O0EbBnyeqPUm0ZLg1LfWJeL9RqPjPhqm\n7w7T94Tpj1Of0OXRMHk5HOidND0wbNOfqIXqf8LnuScsP7GJ92VEiVby+7o4TJc2imEr0CN8fokv\n4WnN7HNXWH5JE+dsSVj2sTA9JUzXhv0n3mMNMCSsk0jIPhCmEwngt4i6zTeE7QuBgUn7+2n4DBKf\n8yNh+4fC9Nz9OcfAB6lP4BI/r4kE5ZpG5zjR4vftMD07TP8sTP+10bG7ATfQ8B+bHxMlQw6c28Rn\nPYn6FtazgeGJ/TX6+fpRmB4AVId5k1s7Xmvvl6j124kS2UQL4GMoQdMjTQ+NQROpd4m7H8jVWqPC\n8/HhkWwsMDq8fs3d6xILfP/GCiWOsTRsW21mq4BBRF2lcQwAcpP3Q/Slto+ZjSNKJHs1sf1Aohaq\nhMQ+ysLz8vC8Kzz3bLT9anffa2ZlSfOWu3utmSWme5pZHVHLYGEzMTQ1L3GsREzLwvNQM8tNWnep\nu1cAmNkeoA9Nv9cBSfNfScxMOmejmjleN2BE0n42uPuG8LqMqKstsd+Hibo/LyNqqRwMPOPupWZ2\nbNL+rm0U29hG0y81irvFc5wU+zCiLtyW9v2fpNhJij3xM5382dQCmFli/5PCo6X94+5LzexW4Dqi\n1lTMbDnwAaLu5YTEz/4WM9tC1HI2POn9NHe8vPC6ufe7Pbxe6+7l4fUbjeMUaS+6ilPk4Fkdnn/k\n7pZ4AIe5+2zgrbD8WDPb97tnZol/lGrDc0u/l4ljTAzb5gBjwry3Y8a5hahrB6Jutn37S3IB0Zdw\nMVF31eCkZdZo3dpWpht71/LEl3ojpxAlZxuIvoS7U58gNI4Boq6xxBdr4v1MCM/r3b0qad3kpNhb\niHULUascJCXdSedsdTPHqwPWxDmeu78B/Jsoqfh2mP3rRvuvImpVTPxM5QKXNIq1slHcrZ3jxL5f\nJ2rRTey7H1GrXrJE/I0/q8TPdPJn082iTDux/780+n0YStT93ICZZQF3uPsAon82vkv0eX6x0aqT\nwvoDiBJRiFrFWjtea+93XVg+3Mzyw+vEBTci7U4Jmki9q8zsx0mPI/dz+/uJulyuM7O/mNkDZvZP\nom4UgN8Sda0dDbxqZj8P9dOOCMsTX+i3heMnt8AkH6MG+JiZzSRqcRlENEj/hThBhmRoZpicaWYP\nEg30T7YxPI8nKtXxTJx9H2SJGAYCPyQa49RUKxcQ+qSjLmCA35vZL4BfhOmftSWAsM//DZO/M7Nf\nh8/9oTDvnvD8EzP7JfDXMP3LRAtdTImE7BSilse/hONvJhoLlQv828zuM7M/Ev2sNHv1cMxzPIco\nwToG+FfY9xyi1tEpMeO+jygxvNjMXjCzB4h+FvsSjUMsAy4xs6fDz/tzIfbBTexrBFAa3t+NRN2S\nUJ+UJ3zWzH5DdIFANlFL75IYx2vt/b5CNOatNzDXzB7l3UmwSLtRgiZS70Kiro/E47D92djdS4Az\niRKJU4EPEf2xvzMs30D0Bfw4UTfLR4m+YBJdK98iSuZOCMd/15eYuxcTjc95GTifqIvpEaIxPVWN\n12/B9USDqPsB04haK6C+FeZRolaH2vCe7tyPfR8U7v4yUcvGTqL3PJP6Vo7mfB34JlFL2oeJBo3f\nQP37a4tbiVpx3iLqbjud+q6ve4kuFFgHXE7UcnYn7+5Ca80j1Ld4PZbUxQZRInZX2PfHgROJzv9T\nreyzxXPs7nvCe5lJ1OX6MaIWq99S303dIndfRDTO7jmifzQ+THQ1bpW7lxJdkTqb6EKY/yb6ub+H\nqIWvsZ1EF26cBHyKqPX0EaKrYpPdRdSdeRjRPygf9EiLx2vt/YZu64tDDEcQJZk/j/M5iKSCRf8g\nikhXYma9gd2hhQgz+xrRQPx/uvspaQ1ODorOdo7N7AWiBOxKd38ovdGIpJ4uEhDpms4AvmFmTwKH\nAFeG+f/b/CbSwegci3RgStBEuqZ3iMohfJmoa60EuNvd/5jWqORg0jkW6cDUxSkiIiKSYXSRgIiI\niEiG6dBdnAMGDPBRo0alOwwRERGRVr3++utb3L2pQtvv0qETtFGjRjFv3rx0hyEiIiLSKjOLW1Bc\nXZwiIiIimUYJmoiIiEiGUYImIiIikmGUoImIiIhkGCVoIiIiIhlGCZqIiIhIhlGCJiIiIpJhlKCJ\niIhIlzbz1Xd4atGGdIfRgBI0ERER6bI27qzg9tlLePw/69IdSgNK0ERERKTL+u5Ty6ipdW4+f1K6\nQ2lACZqIiIh0ScVryvjz/HV84uTRjDwkP93hNKAETURERLocd+e2WYsZ0Ks7154+Nt3hvIsSNBER\nEelyikpKmf9OGV89ZwK9umenO5x3UYImIiIiXcreqlruenIZkwv78IFjhqc7nCYpQRMREZEu5ecv\nrmT9jgpuvWgy3bpZusNpkhI0ERER6TLW79jLfXNXcsGRQzludP90h9MsJWgiIiLSZXz3yWXUOdx0\n3sR0h9IiJWgiIiLSJcx/ZzuPF5dy9SljGNE/s8pqNKYETURERDq9ujrn27OWMKh3dz4z/bB0h9Mq\nJWgiIiLS6T1evI6SNWV89dyJ9MzAshqNpSxBM7MHzWyTmS1KmvcHMysOj9VmVhzmjzKzvUnL7ktV\nXCIiItK1lFfV8N2nljFleF8uPXpYusOJJZUp5EPAz4BfJ2a4+38lXpvZ3cCOpPVXuvvUFMYjIiIi\nXdB9L6xk485K7r3iPRlbVqOxlCVo7v6imY1qapmZGXAZcHqqji8iIiKydns5P39xFTOmFHLMoZlb\nVqOxdI1BOwXY6O5vJs0bHbo355rZKc1taGZXm9k8M5u3efPm1EcqIiIiHdZdTy7DDG7M8LIajaUr\nQbscmJk0vR4YGbo4vwT83sz6NLWhu9/v7tPcfdrAgQPbIVQRERHpiOat3sbsBeu5+tTDGFaQl+5w\n9ku7J2hmlg1cCvwhMc/dK919a3j9OrASGN/esYmIiEjnkCirMaRPD645bUy6w9lv6WhBOxNY5u5r\nEzPMbKCZZYXXY4BxwKo0xCYiIiKdwGPz17Jw3Q5uOm8i+bmZX1ajsVYTNDO73sz6WOSXZjbfzM6O\nsd1M4GVggpmtNbOrwqIP0bB7E+BUYEEou/En4Bp337Z/b0VEREQEdlfW8L2nlzN1RAEzphSmO5w2\niZNSfsLdf2Jm5wD9gI8AvwGeaWkjd7+8mfkfb2LeY8BjMWIRERERadG9f1/B5l2V3P+RYzpMWY3G\n4nRxJt7Z+cBv3H1x0jwRERGRjLFmWzm/+OdbXHL0MI4e2S/d4bRZnATtdTN7hihBe9rMegN1qQ1L\nREREZP/d+eRSssy48dyOVVajsThdnFcBU4FV7l5uZocAV6Y2LBEREZH988qqrcxZuIEvnTWeIX17\npDucA9JqgubudeGOAP9tZg78093/kurAREREROKqrXNum7WEwr49+NQpHa+sRmNxruK8F7gGWAgs\nAj5tZvekOjARERGRuP44bw1L1u/kpvMnkZeble5wDlicLs7TgUnu7gBm9jCwJKVRiYiIiMS0q6Ka\nHzyznGmH9uOio4amO5yDIs5FAiuAkUnTI4A3m1lXREREpF397O8r2LK7ilsuOhyzzlFoIk4LWm9g\nqZm9CjhwHDDPzIoA3H1GCuMTERERadbbW/fwq3+u5v+9ZzhHDS9IdzgHTZwE7ZaURyEiIiLSBnc8\nsZTsLOOr505IdygHVZyrOOea2aHAOHd/zszygGx335X68ERERESa9tKKLTyzZCM3nDOBwX06dlmN\nxuJcxfkpovtj/jzMGg48nsqgRERERFpSW+fcNnsJw/vlcdXJo9MdzkEX5yKBzwEnATsB3P1NYFAq\ngxIRERFpySOvvcOyDbu4+fxJ9Mjp+GU1GouToFW6e1ViwsyyiS4WEBEREWl3O/ZWc/czb3DcqP6c\nd8SQdIeTEnEStLlmdjOQZ2ZnAX8EZqU2LBEREZGm/fT5N9le3rnKajQWJ0G7CdhMdCeBTwNzgG+k\nMigRERGRpqzavJuHXlrNZceM4IhhfdMdTsrEvRfnb4EX3X15O8QkIiIi0qT/mbOUHjlZfOWczlVW\no7E4V3HOAIqBp8L01ESRWhEREZH28o83N/Pc0k187n1jGdi7e7rDSak4XZy3Et09oAzA3YuBznc9\nq4iIiGSsmto6bp+9hJH98/nEyaPSHU7KxUnQqt19R6N5rV7FaWYPmtkmM1uUNO9bZrbOzIrD4/yk\nZV8zsxVmttzMzon/FkRERKSzm/nqO7yxcTc3nz+J7tmdr6xGY3EStMVm9mEgy8zGmdlPgZdibPcQ\ncG4T83/k7lPDYw6AmR0OfAiYHLa518w6/6cvIiIirdpRXs0Pn32DE8YcwjmTB6c7nHYRJ0H7PFHi\nVAnMBHYAX2htI3d/EdgWM46LgUfcvdLd3wJWEHWrioiISBf34+ffYMfear55Yectq9FYnARtqLt/\n3d2Pdfdp7v4Nd684gGN+3swWhC7QfmHeMGBN0jprw7x3MbOrzWyemc3bvHnzAYQhIiIimW7Fpt38\n5uW3+a9jR3J4YZ90h9Nu4iRoD5rZSjN7xMw+Z2ZHHsDx/g8YA0wF1gN37+8O3P3+kChOGzhw4AGE\nIiIiIpnuO08sIS8niy+fPT7dobSrVhM0dz8NmAT8FCgAnjCzuF2Xjfe10d1r3b0OeID6bsx1wIik\nVYeHeSIiItJF/X35Jl5YvpnrzhjHgF6du6xGY60WqjWzk4FTwqMAmA38oy0HM7Oh7r4+TF4CJK7w\nLAJ+b2Y/BAqBccCrbTmGiIiIdHzVtXV8Z/YSRh2Sz8dOHJXucNpdqwka8ALwOnAnMCf5xuktMbOZ\nwHRggJmtJaqnNt3MphKV6VhNdOso3H2xmT0KLAFqgM+5e+1+vRMRERHpNH77ytus3LyHBz46jdzs\nOCOyOpc4CdoA4CTgVOA6M6sDXnb3b7a0kbtf3sTsX7aw/h3AHTHiERERkU5s+54qfvzcm5w8dgBn\nThqU7nDSIs69OMvMbBXRGLHhwIlATqoDExERka7px8+9wa6KrlVWo7E4Y9BWAcuIxp39H3Bl3G5O\nERERkf3xxsZd/Pbf73DF8YcyYUjvdIeTNnG6OMeGqy5FREREUsbduX32EnrmZvHFs7pWWY3G4oy6\nu8vM+phZjpk9b2abzey/Ux6ZiIiIdCl/W7aJf7y5hevPHE//nrnpDiet4iRoZ7v7TuBCoisvxwI3\npDIoERER6Vqqaur4zhNLGTOwJx894dB0h5N2cRK0xAUBFwB/dPcdKYxHREREuqBfv7yat7bs4ZsX\nHE5OVtcrq9FYnDFos8xsGbAX+IyZDQQO5F6cIiIiIvts3V3JT55/k9PGD+R9E7tmWY3G4tzq6Sai\n0hrT3L0aKAcuTnVgIiIi0jX88Nk3KK+q5RsXTEp3KBkjTgsa7r4t6fUeYE/KIhIREZEuY+n6ncx8\n9R0+esIoxg3uumU1GlMnr4iIiKRFoqxGn7wcvnDmuHSHk1GUoImIiEhaPLtkIy+t3MoXzxxPQX7X\nLqvRWKwuTjMbBhyavL67v5iqoERERKRzq6yp5Y45Sxk7qBcfPn5kusPJOHFu9fRd4L+AJUBtmO2A\nEjQRERFpk4f+tZq3t5bz8CeOU1mNJsRpQXs/MMHdK1MdjIiIiHR+m3dV8tO/reD0iYM4bfzAdIeT\nkeKkrKuoL1YrIiIickB++OxyKqpr+brKajQrTgtaOVBsZs8D+1rR3P26lEUlIiIindLi0h088toa\nPnHSaA4b2Cvd4WSsOAlaUXiIiIiItJm7c9usJRTk5XDd6Sqr0ZJWEzR3f9jMcoHxYdbycEeBFpnZ\ng0Q3WN/k7keEed8HLgKqgJXAle5eZmajgKXA8rD5K+5+zX6+FxEREclgTy3awL/f2sbt7z+Cvvka\nPdWSVsegmdl04E3gHuBe4A0zOzXGvh8Czm0071ngCHc/CngD+FrSspXuPjU8lJyJiIh0IhXVUVmN\nCYN7c/mxI9IdTsaL08V5N3C2uy8HMLPxwEzgmJY2cvcXQ8tY8rxnkiZfAT6wP8GKiIhIx/Tgv95i\n7fa9/O6Tx5OtshqtivMJ5SSSMwB3f4ODc1XnJ4Ank6ZHm1mxmc01s1MOwv5FREQkA2zaWcE9f1vB\nmZMGc9LYAekOp0OI04I2z8x+Afw2TF8BzDuQg5rZ14Ea4Hdh1npgpLtvNbNjgMfNbLK772xi26uB\nqwFGjlTlYRERkUz3/aeXU1Vbp7Ia+yFOC9pniO4icF14LAnz2sTMPk508cAV7u4A7l7p7lvD69eJ\nLiAY39T27n6/u09z92kDB6q4nYiISCZbuHYHf5q/litPGs3oAT3THU6HEecqzkrgh+FxQMzsXOCr\nwGnuXp40fyCwzd1rzWwMMI6oQK6IiIh0UO7ObbMX0z8/l2tPH5vucDqUZhM0M3vU3S8zs4VE995s\nIFyJ2SwzmwlMBwaY2VrgVqKrNrsDz5oZ1JfTOBW4zcyqgTrgGnff1ra3JCIiIpngiYXreW31du68\n9Ej69FBZjf3RUgva9eH5wrbs2N0vb2L2L5tZ9zHgsbYcR0RERDJPRXUtd85ZxqShfbhsmspq7K9m\nx6C5+/rw8rPu/nbyA/hs+4QnIiIiHdEDL65iXdlebrnwcLK6WbrD6XDiXCRwVhPzzjvYgYiIiEjn\nsGFHBfe+sJJzJw/hhMMOSXc4HVJLY9A+Q9RSdpiZLUha1Bt4KdWBiYiISMf0vaeXUVvn3Hy+ymq0\nVUtj0H5PVEj2TuCmpPm7NIBfREREmlK8pow/z1/HNacdxshD8tMdTofV0hi0He6+GvgJUQmMxPiz\nGjM7vr0CFBERkY7B3blt1mIG9OqushoHKM4YtP8DdidN7w7zRERERPYpKill/jtlfPWcCfTqHudm\nRdKcOAmaJSr+A7h7HfFuESUiIiJdRHlVDXc9uYwjhvXhA8cMT3c4HV6cBG2VmV1nZjnhcT2q8i8i\nIiJJ7n9xFet3VHDLhZPpprIaByxOgnYNcCKwDlgLHE+4WbmIiIhIadle7pu7kguOHMpxo/unO5xO\nIc69ODcBH2qHWERERKQD+u5Ty6hzuOm8iekOpdNoqQ7aV939e2b2U5q+F+d1KY1MREREMt7rb2/n\nr8WlXPu+sYzor7IaB0tLLWhLw/O89ghEREREOpa6Oue22UsY1Ls7n5l+WLrD6VSaTdDcfVZ4frj9\nwhEREZGO4vHidZSsKeMHH5xCT5XVOKha6uKcRRNdmwnuPiMlEYmIiEjG21NZw3efWsaU4X259Ohh\n6Q6n02kp3f1BeL4UGAL8NkxfDmxMZVAiIiKS2e6bu5KNOyu594r3qKxGCrTUxTkXwMzudvdpSYtm\nmZnGpYmIiHRRa7eXc/+Lq5gxpZBjDlVZjVSIUwetp5mNSUyY2WigZ+pCEhERkUx215PLMFNZjVSK\nM6Lvi8ALZrYKMOBQ4NMpjUpEREQy0murtzF7wXquO2MchQV56Q6n02q1Bc3dnwLGAdcD1wET3P3p\n1rYzswfNbJOZLUqa19/MnjWzN8Nzv6RlXzOzFWa23MzOadvbERERkVSpq3Num7WEIX16cM1pY1rf\nQNqs1QTNzPKBG4Br3b0EGGlmF8bY90PAuY3m3QQ87+7jgOfDNGZ2ONHdCiaHbe41s6y4b0JERERS\n70/z17Jw3Q5uOm8i+bkqq5FKccag/QqoAk4I0+uA77S2kbu/CGxrNPtiIFFX7WHg/UnzH3H3Snd/\nC1gBHBcjNhEREWkHuytr+P7Tyzl6ZAEXTy1MdzidXpwE7TB3/x5QDeDu5URj0dpisLuvD683AIPD\n62HAmqT11oZ572JmV5vZPDObt3nz5jaGISIiIvvj3r+vYPOuSm658HDMVFYj1eIkaFVmlkcoWmtm\nhwGVB3pgd3daKITbwnb3u/s0d582cODAAw1DREREWrFmWzm/+OdbXHL0MI4e2a/1DeSAxelAvhV4\nChhhZr8DTgI+3sbjbTSzoe6+3syGApvC/HXAiKT1hod5IiIikkbuzh1PLCXLjBvPVVmN9tJigmZR\nG+YyorsJvJeoa/N6d9/SxuMVAR8D7grPf02a/3sz+yFQSHTV6KttPIaIiIgcoGUbdlJUXMqsBaWs\n2baXL501niF9e6Q7rC6jxQTN3d3M5rj7kcAT+7NjM5sJTAcGmNlaopa4u4BHzewq4G3gsnCcxWb2\nKLAEqAE+5+61+/tmREREpO3e2VpOUck6ikpKeWPjbrK6GSeNHcAXzhjP+3W/zXYVp4tzvpkd6+6v\n7c+O3f3yZhad0cz6dwB37M8xRERE5MBs3FnB7AXrKSoppWRNGQDHjurH7RdP5rwjhzKgV/c0R9g1\nxUnQjgeuMLO3gT1E3Zzu7kelNDIRERFJibLyKp5ctIGi4lJeeWsr7jC5sA9fO28iF04pZJjuEJB2\ncRI0VfUXERHp4PZU1vDc0o0UFZfy4pubqa51xgzoyXWnj+OiKYWMHdQr3SFKklYTNHd/28zeA5xM\nVBbjX+4+P+WRiYiIyAGprKll7vLNFJWU8tzSjVRU1zG0bw+uPGk0M6YUMrmwj2qaZahWEzQzuwX4\nIPDnMOtXZvZHd2/1bgIiIiLSvmrrnJdXbqWoZB1PLtrArooa+vfM5QPHDGfGlGFMO7Qf3bopKct0\ncbo4rwCmuHsFgJndBRQT43ZPIiIiknruzvx3tlNUXMoTCzewZXclvbpnc/bkwcyYUshJYweQkxWn\nNr1kijgJWinQA6gI091REVkREZG0cneWrt9FUUkps0pKWVe2l9zsbpwxcRAzphTyvomD6JGTle4w\npY3iJGg7gMVm9izRGLSzgFfN7H8B3P26FMYnIiIiSVZv2UNRSSlFJaWs2BTVKjt57AC+dNZ4zp48\nmN49ctIdohwEcRK0v4RHwgupCUVERESasmFHBbMXREnZgrU7ADhuVH9uf/8RnH/EEA5RrbJOJ85V\nnA+3RyAiIiJSb/ueKuYsWk9RcSmvrt6GOxwxrA83nz+RC48qpFC1yjq1OC1oIiIi0g52V9bw7JKo\ngOw/3txCTZ0zZmBPrj9jHDOmFDJmoGqVdRVK0ERERNKoorqWF5ZvZlZJKc8vi2qVFfbtwVUnj+Yi\n1SrrspSgiYiItLOa2jpeWrmVopJSnl60gV2VNRzSM5cPHjOCi6cW8p6RqlXW1TWboJnZLKKrNpvk\n7jNSEpGIiEgnVFcXapWVlDJn4Xq27K6id/dszjliCDOmFHLiYYeQrVplErTUgvaDdotCRESkE3J3\nlqzfSVFJKbNL1rOubC/ds7tx5qTBXDSlkOkTBqpWmTSp2QTN3ee2ZyAiIiKdxVtb9lBUXEpRyTpW\nbt5DdjfjlHED+Mo54znr8CH06q4RRtKylro4F9JyF+dRKYlIRESkA1q/Yy+zS9ZTVFLKwnU7MItq\nlX3i5NHUU55sAAAYS0lEQVScd8RQ+vfMTXeI0oG0lMJf2G5RiIiIdEDb9lQxZ2GUlL0WapUdNbwv\n37hgEhccNZShfVWrTNqmpS7Ot9szEBERkY5gd2UNzyzeQFFJKf8MtcrGDurFF88cz0VTChk9oGe6\nQ5ROoNVOcDPbRX1XZy6QA+xx9z5tOaCZTQD+kDRrDHALUAB8Ctgc5t/s7nPacgwREZGDqaK6lr8v\n20RRSSl/W7aJypo6hhXk8clTxjBjSiGThvZWrTI5qOLc6ql34rVFP30XA+9t6wHdfTkwNewvC1hH\ndK/PK4EfubuuHhURkbSrqa3jnyu2UFRSyjOLN7K7soYBvXL50LEjmBFqlSkpk1TZr8tI3N2Bx83s\nVuCmg3D8M4CV7v62fshFRCTd6uqceW9vp6hkHXMWbmDbnip698jmvCOGMGNqISeMUa0yaR9xujgv\nTZrsBkwDKg7S8T8EzEya/ryZfRSYB3zZ3bc3Ec/VwNUAI0eOPEhhiIhIV+XuLC5N1CorpXRHBT1y\nunHGpMHMCLXKumerVpm0L4saxVpYwexXSZM1wGrgAXffdEAHNssFSoHJ7r7RzAYDW4jGu90ODHX3\nT7S0j2nTpvm8efMOJAwREemiVm7eTVFxKbMWlLIq1Co7dfxAZkwp5KzDB9NTtcrkIDOz1919Wpx1\nW6qD9l13vxF40t0fPWjR1TsPmO/uGwESz+HYDwCzU3BMERHpwkrL9jKrpJSiklIWl+7EDI4f3Z9P\nnjyG844YQj/VKpMM0dK/B+eb2U1EY81SkaBdTlL3ppkNdff1YfISYFEKjikiIl3M1t2VSbXKopEz\nU0KtsguPKmRI3x5pjlDk3VpK0J4CtgO9zGxn0nwjul6gTWU2AMysJ3AW8Omk2d8zs6lEXZyrGy0T\nERGJbVdFNU8v3khRSSn/WrGF2jpn3KBefPmsqFbZKNUqkwzXUqHaG4AbzOyv7n7xwTyou+8BDmk0\n7yMH8xgiItK1VFTX8rdlmygqLuVvyzdRVVPH8H55fPrUMcyYWsiEwapVJh1HnDpoBzU5ExEROViq\nQ62yWcWlPLMkUausOx8+biQzphZy9IgCJWXSIekSFRER6VDq6pzXVm+jqKSUOQvXs728mj49srng\nyKHMmFrIe8ccQlY3JWXSsSlBExGRjOfuLFq3k6KSdcxesJ71OyrIy8nizMOjWmWnjh+gWmXSqbRU\nZuN5dz8jqdyGiIhIu1qxaTdFJaXMKinlrS17yMkyThs/kJvOm8iZk1SrTDqvln6yh5rZicAMM3uE\n6OrNfdx9fkojExGRLmldolZZcSlL1ke1yk4YcwifPnUM5x4xhIJ81SqTzq+lBO0W4JvAcOCHjZY5\ncHqqghIRka5lS6JWWXEp896OapVNHVHALRcezoVHDWVQH9Uqk66lpTIbfwL+ZGbfdPfb2zEmERHp\nAnZWVPP0og0UlZTy0sqt1NY5Ewb35oZzJnDRUYWMPCQ/3SGKpE2cMhu3m9kM4NQw6wV3122YRERk\nv1VU1/L80k38tXgdLyzfTFVtHSP653HNaWOYMWUYE4b0TneIIhmh1QTNzO4EjgN+F2Zdb2YnuvvN\nKY1MREQ6heraOv7x5maKikt5dslG9lTVMrB3d65470hmTClkqmqVibxLnMtfLgCmunsdgJk9DPwH\nUIImIiJNqqtz/v1WVKvsyUXrKSuvpm9eDhdNKWTGlEKOV60ykRbFvT65ANgWXvdNUSwiItKBuTsL\n1u6gqKSU2QtK2bizkrycLM7aV6tsILnZ3dIdpkiHECdBuxP4j5n9najUxqnATSmNSkREOow3N+7a\nV6ts9dbyUKtsEF+/oJAzJw0iP1e1ykT2V5yLBGaa2QvAsWHWje6+IaVRiYhIRluzrZxZC6JaZcs2\n7KKbwQmHHcJnph/GuZOH0jc/J90hinRosf6tcff1QFGKYxERkQy2eVclTywopaiklPnvlAFw9MgC\nbr3ocC44aiiDeqtWmcjBonZnERFp1o69ybXKtlDnMHFIb756blSrbER/1SoTSQUlaCIi0sDeqlqe\nW7qRopJS5oZaZSP75/O5941lxpRCxg1WrTKRVGsxQTOzLGCxu09sp3hERCQNqmpCrbKSqFZZeVUt\ng/t05yMnHMqMKYUcNbyvapWJtKMWEzR3rzWz5WY20t3faa+gREQk9WrrnH+/tZVZJaU8uWgDZeXV\nFOTncPHUYcyYUshxo/urVplImsTp4uwHLDazV4E9iZnuPqOtBzWz1cAuoBaocfdpZtYf+AMwClgN\nXObu29t6DBEReTd3p2TtDoqKo1plm3ZVkp+bxdmHD2bG1EJOHqtaZSKZIE6C9s0UHft97r4lafom\n4Hl3v8vMbgrTN6bo2CIiXcobG3dRVBxdgfnOtnJys7oxfcJAZkwt5IyJg8nLzUp3iCKSJE4dtLlm\ndigwzt2fM7N8IBW/yRcD08Prh4EXUIImItJma7aV7ysgm6hVdtLYAVx7+ljOmTyEvnmqVSaSqeLc\nLP1TwNVAf+AwYBhwH3DGARzXgefMrBb4ubvfDwwO9dYANgCDm4nn6hAPI0eOPIAQREQ6n007K5i9\nYD1FJaUUr4lqlR1zaD++PWMy5x85lIG9u6c5QhGJI04X5+eA44B/A7j7m2Y26ACPe7K7rwv7edbM\nliUvdHc3M29qw5DM3Q8wbdq0JtcREelKdpRX8+SiKCl7ZdVW6hwmDe3DjedO5KIpQxneT7XKRDqa\nOAlapbtXJS6vNrNsohawNnP3deF5k5n9hSgB3GhmQ919vZkNBTYdyDFERDqz8qoanl2ykVklpcx9\nYzPVtc6oQ/K59n1jmTG1kLGDVKtMpCOLk6DNNbObgTwzOwv4LDCrrQc0s55AN3ffFV6fDdxGdCup\njwF3hee/tvUYIiKdUVVNHXPfiGqVPbdkI3uraxnSpwcfO2EUM6YWcuQw1SoT6SziJGg3AVcBC4FP\nA3OAXxzAMQcDfwl/RLKB37v7U2b2GvComV0FvA1cdgDHEBHpFGrrnFdWbaWouJQnF61nZ0UN/fJz\nuOQ9oVbZqP50U60ykU4nzlWcdWb2MNEYNAeWu3ubuzjdfRUwpYn5WzmwCw9ERDoFd+c/a8ooKi7l\niYXr2byrkp65WZw9eQgzphRy8rgB5GSpVplIZxbnKs4LiK7aXAkYMNrMPu3uT6Y6OBGRrmTZhp0U\nFZcya0Epa7btJTe7G++bMJAZU4ZxxqRB9MhRrTKRriJOF+fdREVlVwCY2WHAE4ASNBGR/VBTW8fO\nihq2l1dRVl5NWXhes72cOQvX88bG3WR1M04aO4DrzxjP2ZMH06eHapWJdEVxErRdieQsWEV0myYR\nkS6prs7ZWVFNWXl1lGztrU+2tpdXs6O8iu3l1ZTtTXpdXsXOippm93nsqH7cfvFkzjtyKAN6qVaZ\nSFfXbIJmZpeGl/PMbA7wKNEYtA8Cr7VDbCIiKeXu7KyoYUcTiVYi+dqxt7phi9feanbsraa5kbhm\n0KdHDgX5ORTk51KQn8voAT0pyM+lb14O/fbNj5775efQr2euWspEpIGWWtAuSnq9ETgtvN4M5KUs\nIhGR/eTu7K6s2ZdYle1t2IW4vYl5iUSrtq75a55698imID+HfiG5GtE/P0qw8uqTrH75ufQNzwV5\nOfTJyyFLV1WKyAFqNkFz9yvbMxAREXenvKq2+ZasPYlWrvrWrMR6NS0kWj1zsxokVEML8kKiVd+S\nVZCXQ7+eOfTNi1q1+ublkK0rJUUkTeJcxTka+DwwKnl9d5+RurBEpKOrqK7d1zW4vbwqdCNGLVn7\nuhSTWryisVvVVNXWNbvPvJysKHkKXYPjB/fal1w1TLRy97Vy9c3LITdbiZaIdCxxLhJ4HPgl0d0D\nmv/LKSKdUmVNbYOWrLLyanbsTQx8T+5GjFq5Eq8ra5r/c9E9u1vUJRhaqsYM6JU0ZisntGDlNhiv\n1TcvR2UmRKTLiJOgVbj7/6Y8EhFJqaqaupBARV2D9d2FSVcf7n33lYh7q2ub3WdOlu0b6F6Ql8vI\n/vkcNbx+XFZBorswMUYrPCvREhFpWZwE7SdmdivwDFCZmOnu81MWlYg0q6a2LlxZGFqy9lQ3HLOV\n1F2YfPXhnqrmE63sbtagi3BYQR6TC/vs6y6Mrj5MjNeqv/owLydL934UEUmBOAnakcBHgNOp7+L0\nMC0ibVRb5+zcW1/eoXFCVRaSsLJGpR52tVBLq5vRYEzW4D49mDCk976WrORuxPoB8jn06p6tREtE\nJIPESdA+CIxx96pUByPSEdXVObsqava1XJUlJVlRK1f96+SWrp0VLdfS6ptXX86hf89cDhvYq8mW\nrIK8+i7F3t2zdeNsEZFOIE6CtggoADalOBaRtHJ3dlXWNGjJajzwvWFB0/rWrRYqPNCnR/a+LsG+\n+bkcGmppJa5EbJxoFeTn0LuHammJiHRlcRK0AmCZmb1GwzFoKrMhGcnd2VNV26COVv24rEZXHzYY\nu9Vy0dJe3bP3tVz1y89lWEFeUmtWo1IPYZ0+PbJVS0tERPZbnATt1pRHIdIEd6eiuq6JcVmJUg/N\nX4lYXdt8opWfm7WvMny/njlMHNKnvsswr+EteBKv++blkKNES0RE2kmrCZq7z22PQKRzq6iurW/J\n2tOwnEPZ3irK9jS8+jDxuqqFWlo9crrtS7QK8nMYO6hXk+Oy9rVy5UXlHrpnq8SDiIhktjh3EthF\ndNUmQC6QA+xx9z6pDEwyU1VNXdI9DZsal9X0jaYrqptPtHKzujW4p+GoAfkU5BVQ0LO+RWtf4dKk\neaqlJSIinVWcFrTeidcWXYd/MfDeth7QzEYAvwYGEyV+97v7T8zsW8CniG7GDnCzu89p63GkZdW1\nSUVLy6sbXn24t9HYrT3V+xKt8lZradV3DY7on8+RTYzLSr7RdIFqaYmIiLxLnDFo+7i7A4+HwrU3\ntfGYNcCX3X2+mfUGXjezZ8OyH7n7D9q43y6ppraOnRU1DW/B06hw6fZGdbR2lFezq7L5WlpZ3Wxf\nd2C//FyG9u3BpKF96luykirHJ5d76JmrREtERORgiNPFeWnSZDdgGlDR1gO6+3pgfXi9y8yWAsPa\nur/Ooq7O2VmRdL/DxgPfk+po1V+JWMXOVoqW9k1qrRrYqzvjB/WuvwVPz5wGdbWSa2kp0RIREUmf\nOC1oFyW9rgFWE3VzHjAzGwUcDfwbOAn4vJl9FJhH1Mq2vYltrgauBhg5cuTBCOOgcnd2VtTEGpeV\nfGXijr3NFy0F9g2Ej5KtXEYN6Fl/JWJydfikAfK9e6hoqYiISEdk3lJWkMoDm/UC5gJ3uPufzWww\nsIVoXNrtwFB3/0RL+5g2bZrPmzcvZTHW1jmlZXvfnVAlX33YoJ5WlGi1VEurd/fsBoPf62803XBc\nVnKi1SdPRUtFREQ6OjN73d2nxVm32RY0M7ulhe3c3W/f78jq950DPAb8zt3/HHa4MWn5A8Dstu7/\nYNm5t5pTvvf3Jpf1zM1qMPB9aEFeg0rw+xKsnuHqw/wc+qiWloiIiMTQUhfnnibm9QSuAg4hauXa\nb+FK0F8CS939h0nzh4bxaQCXEN1iKq365OXwvQ8cta91KxogH7V+5WYr0RIREZHUaDZBc/e7E6/D\n1ZbXA1cCjwB3N7ddDCcBHwEWmllxmHczcLmZTSXq4lwNfPoAjnFQZHUzLps2It1hiIiISBfT4kUC\nZtYf+BJwBfAw8J6mBu7vD3f/J9DUgCrVPBMRERGh5TFo3wcuBe4HjnT33e0WlYiIiEgX1tJAqi8D\nhcA3gFIz2xkeu8xsZ/uEJyIiItL1tDQGTaPgRURERNJASZiIiIhIhlGCJiIiIpJhlKCJiIiIZJg4\n9+Ls0r776ndZtm1ZusMQERGRFJrYfyI3HndjusPYRy1oIiIiIhlGLWityKRsWkRERLoGtaCJiIiI\nZBglaCIiIiIZRgmaiIiISIZRgiYiIiKSYZSgiYiIiGQYJWgiIiIiGUYJmoiIiEiGUR201jx5E2xY\nmO4oREREJJWGHAnn3ZXuKPZRC5qIiIhIhsm4FjQzOxf4CZAF/MLd05vOZlA2LSIiIl1DRrWgmVkW\ncA9wHnA4cLmZHZ7eqERERETaV0YlaMBxwAp3X+XuVcAjwMVpjklERESkXWVagjYMWJM0vTbM28fM\nrjazeWY2b/Pmze0anIiIiEh7yLQErVXufr+7T3P3aQMHDkx3OCIiIiIHXaYlaOuAEUnTw8M8ERER\nkS4j0xK014BxZjbazHKBDwFFaY5JREREpF1lVJkNd68xs2uBp4nKbDzo7ovTHJaIiIhIu8qoBA3A\n3ecAc9Idh4iIiEi6mLunO4Y2M7PNwNvtcKgBwJZ2OI6khs5fx6dz2PHpHHZ8OocH7lB3j3WFY4dO\n0NqLmc1z92npjkPaRuev49M57Ph0Djs+ncP2lWkXCYiIiIh0eUrQRERERDKMErR47k93AHJAdP46\nPp3Djk/nsOPTOWxHGoMmIiIikmHUgiYiIiKSYZSgiYiIiGQYJWiBmZ1rZsvNbIWZ3dTE8ulmtsPM\nisPjlnTEKc1r7RyGdaaH87fYzOa2d4zSshi/hzck/Q4uMrNaM+ufjljl3WKcv75mNsvMSsLv4JXp\niFOaF+Mc9jOzv5jZAjN71cyOSEecXYHGoAFmlgW8AZwFrCW6J+jl7r4kaZ3pwFfc/cK0BCktinkO\nC4CXgHPd/R0zG+Tum9ISsLxLnHPYaP2LgC+6++ntF6U0J+bv4M1AX3e/0cwGAsuBIe5elY6YpaGY\n5/D7wG53/7aZTQTucfcz0hJwJ6cWtMhxwAp3XxX+UDwCXJzmmGT/xDmHHwb+7O7vACg5yzj7+3t4\nOTCzXSKTOOKcPwd6m5kBvYBtQE37hiktiHMODwf+BuDuy4BRZja4fcPsGpSgRYYBa5Km14Z5jZ0Y\nmnWfNLPJ7ROaxBTnHI4H+pnZC2b2upl9tN2ikzji/h5iZvnAucBj7RCXxBPn/P0MmASUAguB6929\nrn3CkxjinMMS4FIAMzsOOBQY3i7RdTEZd7P0DDYfGOnuu83sfOBxYFyaY5L9kw0cA5wB5AEvm9kr\n7v5GesOSNrgI+Je7b0t3ILJfzgGKgdOBw4Bnzewf7r4zvWHJfrgL+ImZFRMl2f8BatMbUuekFrTI\nOmBE0vTwMG8fd9/p7rvD6zlAjpkNaL8QpRWtnkOi/wafdvc97r4FeBGY0k7xSevinMOED6HuzUwT\n5/xdSTTMwN19BfAWMLGd4pPWxf0uvNLdpwIfBQYCq9ovxK5DCVrkNWCcmY02s1yiP/5FySuY2ZAw\nbiLRrNsN2NrukUpzWj2HwF+Bk80sO3SRHQ8sbec4pXlxziFm1hc4jeh8SuaIc/7eIWrBJoxbmoC+\n3DNJnO/CgrAM4JPAi2oBTQ11cQLuXmNm1wJPA1nAg+6+2MyuCcvvAz4AfMbMaoC9wIdcl8BmjDjn\n0N2XmtlTwAKgDviFuy9KX9SSLObvIcAlwDPuvidNoUoTYp6/24GHzGwhYMCNoTVbMkDMczgJeNjM\nHFgMXJW2gDs5ldkQERERyTDq4hQRERHJMErQRERERDKMEjQRERGRDKMETURERCTDKEETERERyTBK\n0ESk0zCzwWb2ezNbFW7n9bKZXZLuuERE9pcSNBHpFEIh6ceJCmeOcfdjiApt6j6BItLhKEETkc7i\ndKAqqaAt7v62u//UzEaZ2T/MbH54nAhgZtPNbK6Z/TW0ut1lZleY2atmttDMDgvrPWRm/2dmr4T1\nppvZg2a21MweShwvrDPPzBab2bfb+wMQkc5DdxIQkc5iMjC/mWWbgLPcvcLMxhHdx3NaWDaFqDr6\nNqLbDv3C3Y8zs+uBzwNfCOv1A04AZhDd/uYkolvdvGZmU929GPi6u28zsyzgeTM7yt0XHPR3KiKd\nnlrQRKRTMrN7zKzEzF4DcoAHwi2G/ggcnrTqa+6+3t0rgZXAM2H+QmBU0nqzwu3dFgIb3X2hu9cR\n3e4msd5lZjYf+A9Rwph8HBGR2NSCJiKdxWLg/yUm3P1zZjYAmAd8EdhI1FrWDahI2q4y6XVd0nQd\nDf9GVjaxzr71zGw08BXgWHffHro+exzgexKRLkotaCLSWfwN6GFmn0malx+e+wLrQ4vXR4huBH2w\n9QH2ADvMbDBwXgqOISJdhFrQRKRTcHc3s/cDPzKzrwKbiRKmG4nGpj1mZh8FngrzD/bxS8zsP8Ay\nYA3wr4N9DBHpOiwaUiEiIiIimUJdnCIiIiIZRgmaiIiISIZRgiYiIiKSYZSgiYiIiGQYJWgiIiIi\nGUYJmoiIiEiGUYImIiIikmH+Pwe3kT+JXmeMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a988ff5b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-params\n",
    "gamma = 0.99\n",
    "epsilon = 1e-4\n",
    "\n",
    "# The GRIDWORLD\n",
    "world_size = 5\n",
    "terminal_states = [(0,0), (world_size-1, world_size-1), (world_size-2, world_size-3), (2, world_size-int(world_size/2))]\n",
    "actions, states, nextState = create_gridworld(world_size, terminal_states)\n",
    "\n",
    "# Initializations\n",
    "V_init = np.zeros((world_size, world_size), dtype=np.float)    # V(s) ... our value function estimate for PI\n",
    "PI_init = np.random.randint(low=0, high=4, size=(world_size, world_size), dtype=np.int)     # PI(s) ... our greedy policy\n",
    "\n",
    "print(\"INITIALIZATION\")\n",
    "print(\"Initial value function V is filled with zeros whereas initial policy is random\")\n",
    "print(\"\\nV = \\n\", np.round(V_init))\n",
    "print(\"\\nPI = \")\n",
    "print_policy(PI_init, terminal_states)\n",
    "\n",
    "PolIt_results = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR POLICY ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(PolIt_results[1]), PolIt_results[1]))\n",
    "print(\"\\nV = \\n\", np.round(PolIt_results[0]))\n",
    "print(\"\\nPI = \")\n",
    "print_policy(PolIt_results[2], terminal_states)\n",
    "\n",
    "ValIt_results = value_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR VALUE ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations\".format(ValIt_results[1]))\n",
    "print(\"\\nV = \\n\", np.round(ValIt_results[0]))\n",
    "print(\"\\nPI = \")\n",
    "print_policy(ValIt_results[2], terminal_states)\n",
    "\n",
    "M_PolIt_results = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, gamma, epsilon, modified=True)\n",
    "\n",
    "print(\"\\n\\nRESULTS FOR MODIFIED POLICY ITERATION -------------\")\n",
    "print(\"Policy found in {} iterations, where each policy evaluation lasted for k = {}\".format(len(M_PolIt_results[1]), M_PolIt_results[1]))\n",
    "print(\"\\nV = \\n\", np.round(M_PolIt_results[0]))\n",
    "print(\"\\nPI = \")\n",
    "print_policy(M_PolIt_results[2], terminal_states)\n",
    "\n",
    "print(\"\\n\\nEFFECT OF GAMMA ON CONVERGENCE SPEED\")\n",
    "\n",
    "logg = {\"policy_iteration\": [], \"value_iteration\": [], \"M_policy_iteration\": []}\n",
    "# For different values of gamma\n",
    "gammas = [0.50, 0.75, 0.90, 0.95]\n",
    "for g in gammas:\n",
    "    \n",
    "    # Run Policy Iteration\n",
    "    _, PolIt_k, _ = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, g, epsilon)\n",
    "    logg['policy_iteration'].append(sum(PolIt_k))\n",
    "    \n",
    "    # Run Value Iteration\n",
    "    _, ValIt_k, _ = value_iteration(V_init, PI_init, world_size, states, actions, nextState, g, epsilon)\n",
    "    logg['value_iteration'].append(ValIt_k)\n",
    "    \n",
    "    # Run Modified Policy Iteration\n",
    "    _, M_PolIt_k, _ = policy_iteration(V_init, PI_init, world_size, states, actions, nextState, g, epsilon, modified=True)\n",
    "    logg['M_policy_iteration'].append(sum(M_PolIt_k))\n",
    "    \n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(gammas, logg['policy_iteration'], label=\"Policy Iteration\")\n",
    "plt.plot(gammas, logg['value_iteration'], label=\"Value Iteration\")\n",
    "plt.plot(gammas, logg['M_policy_iteration'], label=\"Modified Policy Iteration\")\n",
    "plt.title('Effect of gamma on convergence speed', fontweight='bold')\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Number of full prediction sweeps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Bl abla bla bla bla"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
