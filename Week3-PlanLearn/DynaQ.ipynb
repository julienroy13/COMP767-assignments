{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycolab import ascii_art\n",
    "from pycolab.prefab_parts import sprites as prefab_sprites\n",
    "from pycolab.rendering import ObservationToFeatureArray\n",
    "\n",
    "import numpy as np\n",
    "import pycolab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Blocking Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_LOCATION = (1, 9)\n",
    "GAME_ART = ['###########',\n",
    "            '#         #',\n",
    "            '#         #',\n",
    "            '#         #',\n",
    "            '######### #',\n",
    "            '#         #',\n",
    "            '#   P     #',\n",
    "            '###########']\n",
    "\n",
    "\n",
    "def make_game():\n",
    "    \"\"\"Builds and returns game.\"\"\"\n",
    "    return ascii_art.ascii_art_to_game(GAME_ART, what_lies_beneath=' ', sprites={'P': PlayerSprite})\n",
    "\n",
    "\n",
    "class PlayerSprite(prefab_sprites.MazeWalker):\n",
    "    \"\"\"A `Sprite` for our player.\n",
    "    This `Sprite` ties actions to going in the four cardinal directions. If we\n",
    "    reach a magical location, the agent receives a reward of 1 and the epsiode terminates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corner, position, character):\n",
    "        \"\"\"Inform superclass that the '#' delimits the walls.\"\"\"\n",
    "        super(PlayerSprite, self).__init__(corner, position, character, impassable='#')\n",
    "    \n",
    "\n",
    "\n",
    "    def update(self, actions, board, layers, backdrop, things, the_plot):\n",
    "        del layers, backdrop, things   # Unused in this application.\n",
    "\n",
    "        # Apply motion commands.\n",
    "        if actions == 0:    # walk upward?\n",
    "            self._north(board, the_plot)\n",
    "            #print('Walks up')\n",
    "        elif actions == 1:  # walk downward?\n",
    "            self._south(board, the_plot)\n",
    "            #print('Walks down')\n",
    "        elif actions == 2:  # walk leftward?\n",
    "            self._west(board, the_plot)\n",
    "            #print('Walks left')\n",
    "        elif actions == 3:  # walk rightward?\n",
    "            self._east(board, the_plot)\n",
    "            #print('Walks right')\n",
    "\n",
    "        # See if we've found the mystery spot.\n",
    "        if self.position == GOAL_LOCATION:\n",
    "            the_plot.add_reward(1.0)\n",
    "            the_plot.terminate_episode()\n",
    "        else:\n",
    "            the_plot.add_reward(0.0)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_board(obs):\n",
    "    \n",
    "    board = 10 * np.array(obs.layers['P'], dtype=np.float)\n",
    "    board += 2 * np.array(obs.layers['#'], dtype=np.float)\n",
    "    \n",
    "    goal_mask = np.zeros(shape=board.shape)\n",
    "    goal_mask[GOAL_LOCATION] = 1\n",
    "    board += 7 * goal_mask\n",
    "\n",
    "    plt.figure(figsize=(2,1))\n",
    "    plt.imshow(board)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciates our game object\n",
    "game = make_game()\n",
    "\n",
    "# Finalize the engine. Set-up and compute the first observation of the game\n",
    "obs, reward, gamma = game.its_showtime();\n",
    "print(reward, gamma)\n",
    "\n",
    "# Take actions at random until termination\n",
    "while not(game.game_over):\n",
    "    a = np.random.randint(4)\n",
    "    obs, reward, gamma = game.play(a)\n",
    "    print(reward, gamma)\n",
    "    show_board(obs)\n",
    "print('GAME OVER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Shortcut Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynaQ():\n",
    "    \n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(object):\n",
    "    def __init__(self):\n",
    "        self._mapping = dict()\n",
    "    \n",
    "    def feed(self, state, action, next_state, reward):\n",
    "        if tuple(state) not in self._mapping.keys():\n",
    "            self._mapping[tuple(state)] = dict()\n",
    "        self._mapping[tuple(state)][action] = (reward, list(next_state))\n",
    "    \n",
    "    def sample(self):\n",
    "        #\n",
    "        state_index = np.random.choice(range(0, len(self._mapping.keys())))\n",
    "        state = list(self._mapping)[state_index]\n",
    "        #\n",
    "        action_index = np.random.choice(range(0, len(self._mapping[state].keys())))\n",
    "        action = list(self._mapping[state])[action_index]\n",
    "        reward, next_state = self._mapping[state][action]\n",
    "        return list(state), action, reward, list(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ(object):\n",
    "    def __init__(self):\n",
    "        # discount factor\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        # probability for exploration\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # step size\n",
    "        self.alpha = 0.1\n",
    "\n",
    "        # n-step planning\n",
    "        self.n_planning_steps = 5\n",
    "\n",
    "        # average over several independent runs\n",
    "        self.runs = 10\n",
    "        \n",
    "    # action selection with epsilon-greedy scheme\n",
    "    def _select_action(self, state, state_action_values):\n",
    "        if np.random.binomial(1, self.epsilon) == 1:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            values = state_action_values[state[0], state[1], :]\n",
    "            return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])\n",
    "\n",
    "    # tabular dyna-Q algorithm\n",
    "    def apply(self, state_action_values, model, game):\n",
    "        #\n",
    "        obs, reward, gamma = game.its_showtime()\n",
    "        position = np.array(obs.layers['P'])\n",
    "        current_state = np.unravel_index(position.argmax(), position.shape)\n",
    "\n",
    "        steps = 0\n",
    "        while not(game.game_over):\n",
    "            #\n",
    "            steps += 1\n",
    "\n",
    "            # choose an action to execute\n",
    "            action = self._select_action(current_state, state_action_values)\n",
    "\n",
    "            # take action\n",
    "            obs, reward, gamma = game.play(action)\n",
    "            position = np.array(obs.layers['P'])\n",
    "            next_state = np.unravel_index(position.argmax(), position.shape)\n",
    "\n",
    "            # Q-Learning update\n",
    "            state_action_values[current_state[0], current_state[1], action] += \\\n",
    "                self.alpha * (reward + self.gamma * np.max(state_action_values[next_state[0], next_state[1], :]) -\n",
    "                state_action_values[current_state[0], current_state[1], action])\n",
    "\n",
    "            # feed the model with experience\n",
    "            model.feed(current_state, action, next_state, reward)\n",
    "\n",
    "            # sample experience from the model\n",
    "            for t in range(0, self.n_planning_steps):\n",
    "                sample_state, sample_action, sample_reward, sample_next_state = model.sample()\n",
    "                state_action_values[sample_state[0], sample_state[1], sample_action] += \\\n",
    "                    self.alpha * (sample_reward + self.gamma * np.max(state_action_values[sample_next_state[0], sample_next_state[1], :]) -\n",
    "                    state_action_values[sample_state[0], sample_state[1], sample_action])\n",
    "\n",
    "            current_state = next_state\n",
    "            show_board(obs)\n",
    "        print('GAME OVER')\n",
    "\n",
    "        return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate game\n",
    "game = make_game()\n",
    "\n",
    "# initialize state action values\n",
    "state_action_values = np.zeros((game.rows, game.cols, 4))\n",
    "\n",
    "# instantiate simple model\n",
    "model = SimpleModel()\n",
    "\n",
    "# instantiate dyna-Q algorithm\n",
    "algorithm = DynaQ()\n",
    "\n",
    "# apply algorithm\n",
    "algorithm.apply(state_action_values, model, game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How dows Dyna-Q relate to Experience Replay ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
