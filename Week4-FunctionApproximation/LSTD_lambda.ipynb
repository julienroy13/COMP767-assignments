{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximation in RL : LSTD(lambda)\n",
    "\n",
    "**Due date : 20/03/2018**\n",
    "\n",
    "**By : Julien Roy and David Kanaa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pycolab\n",
    "\n",
    "from pycolab import ascii_art\n",
    "from pycolab.prefab_parts import sprites as prefab_sprites\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Squares Temporal Difference methods[2] (LSTD) allow to make better use of the training data than simpler TD learning methods at the cost of additional computation. However, in many setups, these additional computations are worth it as LSTD will converge faster overall than TD methods. The LSTD($\\lambda$) algorithm from Boyan[1] adds eligibility traces to this process, allowing to choose the amount of bootstrapping to be used.\n",
    "\n",
    "In this work, we implement two versions of this algorithm : an off-line version (complexity $O(n^3)$ in space) and a recursive version inspired by Bradtke and Barto [2] (complexity $O(n^2)$ in space). Those two algorithms are tested in two simple environments : the 13-states Boyan's Chain [1] and the 5-states Bradtke's Domain [2]. Finally, we empirically validate a proof made in Boyan's paper demonstrating that when $\\lambda=1$, LSTD($\\lambda$) is equivalent to supervised learning using a least squared error loss and sampled Monte-Carlo trajectories.\n",
    "\n",
    "[1] : Bradtke, S. J., & Barto, A. G. (1996). Linear least-squares algorithms for temporal difference learning. Machine learning, 22(1-3), 33-57.\n",
    "\n",
    "[2] : Boyan, J. A. (2002). Technical update: Least-squares temporal difference learning. Machine Learning, 49(2-3), 233-246."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_board(obs, board_footprints=None, leave_footprints=False):\n",
    "    \n",
    "    # Represents the agent as a yellow dot on the board\n",
    "    board = 100 * np.array(obs.layers['P'], dtype=np.float)\n",
    "    \n",
    "    # Indicates in which states the agent went\n",
    "    if leave_footprints:\n",
    "        new_footprints = 20 * np.array(obs.layers['P'], dtype=np.float)\n",
    "    \n",
    "        if board_footprints is not None:\n",
    "            board_footprints += new_footprints\n",
    "            board += board_footprints\n",
    "        else:\n",
    "            board_footprints = new_footprints\n",
    "    \n",
    "    # Adds the Goal as a green dot on the board (if there is one)\n",
    "    if 'G' in obs.layers:\n",
    "        board += 80 * np.array(obs.layers['G'], dtype=np.float)\n",
    "\n",
    "    # Creates the figure\n",
    "    plt.figure(figsize=5 * np.array(board.shape))\n",
    "    plt.imshow(board)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return board_footprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment - Boyan's Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOYAN_CHAIN = ['P           G']\n",
    "\n",
    "boyan_features = np.array([\n",
    "            [1.00, 0.00, 0.00, 0.00],\n",
    "            [0.75, 0.25, 0.00, 0.00],\n",
    "            [0.50, 0.50, 0.00, 0.00],\n",
    "            [0.25, 0.75, 0.00, 0.00],\n",
    "            [0.00, 1.00, 0.00, 0.00],\n",
    "            [0.00, 0.75, 0.25, 0.00],\n",
    "            [0.00, 0.50, 0.50, 0.00],\n",
    "            [0.00, 0.25, 0.75, 0.00],\n",
    "            [0.00, 0.00, 1.00, 0.00],\n",
    "            [0.00, 0.00, 0.75, 0.25],\n",
    "            [0.00, 0.00, 0.50, 0.50],\n",
    "            [0.00, 0.00, 0.25, 0.75],\n",
    "            [0.00, 0.00, 0.00, 1.00],\n",
    "        ])\n",
    "\n",
    "def make_BoyanChain(art):\n",
    "    \"\"\"Builds and returns game.\"\"\"\n",
    "    return ascii_art.ascii_art_to_game(art, what_lies_beneath=' ', sprites={'P': PlayerSprite_BoyanChain})\n",
    "\n",
    "class PlayerSprite_BoyanChain(prefab_sprites.MazeWalker):\n",
    "\n",
    "    def __init__(self, corner, position, character):\n",
    "        \"\"\"Inform superclass that the '#' delimits the walls.\"\"\"\n",
    "        super(PlayerSprite_BoyanChain, self).__init__(corner, position, character, impassable='#')\n",
    "    \n",
    "    def update(self, actions, board, layers, backdrop, things, the_plot):\n",
    "        del backdrop, things   # Unused in this application.\n",
    "        _, position = self.position\n",
    "        \n",
    "        if actions == 0: # Fake action.. just to avoid making a move during its_showtime() call\n",
    "            \n",
    "            # From the last state before the Goal\n",
    "            if layers[\"G\"][0, position + 1]:\n",
    "                self._east(board, the_plot)   # single jump east\n",
    "                the_plot.add_reward(-2.0)\n",
    "\n",
    "            # From any other state \n",
    "            else:\n",
    "                # Each one of the 2 possible transitions have a probability of 0.5\n",
    "                if np.random.rand() > 0.5:    # single jump east\n",
    "                    self._east(board, the_plot)\n",
    "                else:                         # double jump east\n",
    "                    self._east(board, the_plot)\n",
    "                    self._east(board, the_plot)\n",
    "\n",
    "                # Any transition from those states give a reward of -3\n",
    "                the_plot.add_reward(-3.0)\n",
    "                 \n",
    "        # Check if our agent is on the goal position\n",
    "        if layers[\"G\"][self.position]:\n",
    "            the_plot.terminate_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAQNJREFUeJzt3bEJAkEQQFFPrMLcVEyswc4sx1oswyZ0LUC+GNyhyHvx\nskz0mWTZaYyxAuDV+tsDAPwqgQQIAgkQBBIgCCRAEEiAIJAAQSABgkAChM23B3jncdvN/szntD3M\nfSWwkOP1vsi95/1l+uScDRIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJA\nAgSBBAgCCRAEEiAIJEAQSIAgkABBIAHCNMbsHwcC/AUbJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIg\nCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRCe83wNa2oLT7cA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585a0d5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAARhJREFUeJzt3bFNQzEUQFF/xBSpGYCaHpG9UrAREhtEVFGGoGcCM0B0\nEYVRAJ1TW0+vunJjeZtzDgAu3Vx7AYDfSiABgkACBIEECAIJEAQSIAgkQBBIgCCQAOH22gt85fHh\nsP6Zz/G8fOQYY7y+n5bPfNrdL58Jf8nHy92PzH3bP2/fOecGCRAEEiAIJEAQSIAgkABBIAGCQAIE\ngQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAGGbc/3HgQD/gRskQBBI\ngCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSAB\ngkACBIEECAIJED4Bb0wQa/06u+AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585a0d5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAR9JREFUeJzt3bFJBUEYRtF/xSqMjcVQzEX7MrCPV4ChYAdiaCdWsBYg\nVwzm8UTOiZePiS6TLLPt+z4AfHd26gMA/FUCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQI56c+wE/u\nbh/X/+bz9rF8cmZmbq6WT74+H5ZvzszcX1wfZRdW+3y5PMru+8PT9pvv3CABgkACBIEECAIJEAQS\nIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgbPu+/uFA\ngP/ADRIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAI\nJEAQSIAgkABBIAGCQAIEgQQIX5JxEGuDVaWFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585abc36d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAR1JREFUeJzt3bFJBEEAhtFZsQpjYzEUc9G+DOzjCjA8uA7E0E6sYCxA\nPjGY5UTei5efiT4mWWabcw4Avrs49wEA/iqBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAKEy3Mf4CcP\n98/rf/N5+1g+OcYY4+5mn90dnF4Pyzcfr26Xb8Ln8XqX3fenl+0337lBAgSBBAgCCRAEEiAIJEAQ\nSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQNjmXP9wIMB/\n4AYJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIg\nCCRAEEiAIJAAQSABgkAChC+vthBrTCtxugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585b982ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAASBJREFUeJzt3bFJBEEAhtFZsQpjYzEUc9G+LrCPK8BQsIPjQjuxgrEA\n+eSCXc6T9+LlZ6KPSZZZ5pwDgJ+uzn0AgL9KIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAA4frcB/jN\n0+Nu/d98Dp+rT44xxni422b3Qny87TfZfb6532SXy/D1frvJ7vHldTnlOzdIgCCQAEEgAYJAAgSB\nBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECMuc6z8c\nCPAfuEECBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIE\ngQQIAgkQBBIgCCRAEEiAIJAA4RvS2xBr9GW+FwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585abefe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAASRJREFUeJzt3TFKxFAYRtE/4iqsrcVS7EX3ZeGOBtyBWImLsHcFcQFy\nxSJhxDmnDh8PApfXhCzrug4A350d+wAAf5VAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAHC+bEP8JO7\n28ftP/N5ed98cmZmbq722T11O7yv54+3zTdnZu4vrnfZPWWfh8tddl8fnpbfPOcGCRAEEiAIJEAQ\nSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAGFZ\n1+1/HAjwH7hBAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSAB\ngkACBIEECAIJEAQSIAgkQBBIgCCQAOELpCoSa2NrhkoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585c117c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAASlJREFUeJzt3bFJBUEUQNFZsQpjYzH8mIv2ZWBHgh3Ij8QizK1gLECu\nGOzwRc+Jl8eDgcskw25zzgHAV2enXgDgtxJIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEA4P/UC37m9\nedj/mc/L2+4jxxhjHK7WzP3vVpzXqrNasOvz++vuM8cY4+7iesncvX08XS6Ze7x/3H7ynRskQBBI\ngCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSAB\ngkAChG3O/X8cCPAXuEECBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAg\nkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAA4RMdahRrKPJekQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585c7e0198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAASlJREFUeJzt3b1JBUEUgNFZsQpjYzEUc9G+DOzjFWAo2IEY2okVjAXI\nJwazPH/OiZfLhYGPSYbd5pwDgM9Ojr0AwE8lkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAcHrsBb5y\nc32//pnPy9vykWOMMa4u9pn73+1xXnud1S/a9fnxsHzm7dnl8pnvT+fLZ44xxuvdw/ad79wgAYJA\nAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJ\nEAQSIGxzrv9xIMBf4AYJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGC\nQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkAChA86rxRrXuv/pAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585c803780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAA4CAYAAACfQWFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAR9JREFUeJzt3bFNxEAQQNHx6YqgCmJEfpVRFR0QIlJy2jAFoI8IvDo4\n3out8VgrfTmxvO37PgB8dbr2AgC/lUACBIEECAIJEAQSIAgkQBBIgCCQAEEgAcL52gt85/L4dPxn\nPi9vh4+cmZmH+zVz/7sV57XqrP7SrisseP7nj9fDZ87MnO7etx9dt+TuADdAIAGCQAIEgQQIAgkQ\nBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRC2fT/+x4EA\nt8AbJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBI\ngCCQAEEgAYJAAgSBBAgCCRA+AReDE2vq4WfdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585c08eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME OVER\n",
      "Undiscounted Return : -24.0\n"
     ]
    }
   ],
   "source": [
    "undiscounted_return = 0\n",
    "\n",
    "game = make_BoyanChain(BOYAN_CHAIN)\n",
    "obs, reward, gamma = game.its_showtime()\n",
    "board_footprints = show_board(obs, leave_footprints=True)\n",
    "while not(game.game_over):\n",
    "    obs, reward, gamma = game.play(0)\n",
    "    undiscounted_return += reward\n",
    "    board_footprints = show_board(obs, board_footprints, leave_footprints=True)\n",
    "print('GAME OVER')\n",
    "print('Undiscounted Return : {}'.format(undiscounted_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment - Bradtke's 5-states Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BRADTKE_DOMAIN = ['P    ']\n",
    "\n",
    "def make_BradtkeDomain(art):\n",
    "    \"\"\"Builds and returns game.\"\"\"\n",
    "    return ascii_art.ascii_art_to_game(art, what_lies_beneath=' ', sprites={'P': PlayerSprite_BradtkeDomain})\n",
    "\n",
    "class PlayerSprite_BradtkeDomain(prefab_sprites.MazeWalker):\n",
    "\n",
    "    def __init__(self, corner, position, character):\n",
    "        \"\"\"Inform superclass that the '#' delimits the walls.\"\"\"\n",
    "        super(PlayerSprite_BradtkeDomain, self).__init__(corner, position, character, impassable='#')\n",
    "        \n",
    "        self.transition_matrix = np.array([\n",
    "            [0.42, 0.13, 0.14, 0.03, 0.28],\n",
    "            [0.25, 0.08, 0.16, 0.35, 0.15],\n",
    "            [0.08, 0.20, 0.33, 0.17, 0.22],\n",
    "            [0.36, 0.05, 0.00, 0.51, 0.07],\n",
    "            [0.17, 0.24, 0.19, 0.18, 0.22]\n",
    "        ])\n",
    "        \n",
    "        self.state_features = np.array([\n",
    "            [74.29, 34.61, 73.48, 53.29,  7.79],\n",
    "            [61.60, 48.07, 34.68, 36.19, 82.02],\n",
    "            [97.00,  4.88,  8.51, 87.89,  5.17],\n",
    "            [41.10, 40.13, 64.63, 92.67, 31.09],\n",
    "            [ 7.76, 79.82, 43.78,  8.56, 61.11]\n",
    "        ])\n",
    "        \n",
    "        self.reward_matrix = np.array([\n",
    "            [104.66, 29.69,  82.36,  37.49, 68.82],\n",
    "            [ 75.86, 29.24, 100.37,   0.31, 35.99],\n",
    "            [ 57.68, 65.66,  56.95, 100.44, 47.63],\n",
    "            [ 96.23, 14.01,   0.88,  89.77, 66.77],\n",
    "            [ 70.35, 23.69,  73.41,  70.70, 85.41]\n",
    "        ])\n",
    "    \n",
    "    def update(self, actions, board, layers, backdrop, things, the_plot):\n",
    "        del backdrop, things   # Unused in this application.\n",
    "        _, position = self.position\n",
    "        \n",
    "        if actions == 0: # Fake action.. just to avoid making a move during its_showtime() call\n",
    "\n",
    "            # New position is sampled from a multinouilli distribution parametrized by the environment's transition matrix\n",
    "            new_position = np.argmax(np.random.multinomial(n=1, pvals=self.transition_matrix[position, :]))\n",
    "            \n",
    "            # Receives a reward according to the reward matrix (deterministic reward associated with each transition)\n",
    "            the_plot.add_reward(self.reward_matrix[position, new_position])\n",
    "            \n",
    "            # Move the agent to the new position\n",
    "            self._teleport((0, new_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAABbCAYAAAARIRyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAWRJREFUeJzt27ENAjEUBUGMqIKcnC4olhJoiCrOFIAWkfkEM7GDF61+\n4jHnPADw7rh6AMBeCSRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQDitHvDJ9rz89D/I2/m6egL8\npcd2H9+8c0ECBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGC\nQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgC\nCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgk\nQBBIgCCQAGHMOVdvANglFyRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSB\nBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQXjTWDLHYXoXnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585a4834e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAABbCAYAAAARIRyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAXpJREFUeJzt28FpQkEYRlGfWIVVZO0+WJjlpAJJA1mKRaQMxwLkBkEe\nI+Gc9Sy+1eXfzDLG2ADwaDt7AMC7EkiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgLCbPeAvn4fT\n//4H+XOdvWBV59/L7AmrOe4/Zk/gBd+3r+WZdy5IgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAg\nkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJA\nAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJ\nEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAsY4zZGwDekgsSIAgkQBBIgCCQAEEgAYJAAgSB\nBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECHfr9A+x\nEPHkAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585c8135c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAABbCAYAAAARIRyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAYBJREFUeJzt2zFqQkEYRlGfuAoXESzFPriw7CRtwD5kA9YuyMkC5Eqa\nx0g4p57iqy5/M8sYYwPAo+3sAQCvSiABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAcJu9oBn3k8f\n//sf5PU2e8G6jm+zF6zm+/I5e8KqzvvD7Amr+rl/LX9554IECAIJEAQSIAgkQBBIgCCQAEEgAYJA\nAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJ\nEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRA\nEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAcIyxpi9AeAluSABgkACBIEECAIJEAQS\nIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiA\n8AutOg+xvqh4zgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585a467978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAABbCAYAAAARIRyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAXdJREFUeJzt27GpQkEURVGfWIW5iZFdWKwl2ITRr8Eq3liAbBFBRj5r\nxROcaHOTWcYYGwCebWcPAPhVAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEHazB7yy3g//+h/k\neX+aPeGrtqfj7Al8aL39zZ7wVdf1srzzzgUJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAI\nJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQ\nAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkAC\nBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAoRljDF7A8BPckECBIEECAIJEAQSIAgkQBBIgCCQAEEg\nAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAA4QG90w+xi7W5\nlAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585b9de0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAABbCAYAAAARIRyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAX1JREFUeJzt27FtQkEURUF/RAkEVGHJGSW5GgogpADnroAiKGZdgHUQ\nQvpaQDPxBjc6eskuY4wPAP7bzB4A8KwEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgbGcPuOXr\n+/jW/yB3p8vsCes6fM5ewIN+f86zJ6xqs78ud71bewjAqxJIgCCQAEEgAYJAAgSBBAgCCRAEEiAI\nJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQ\nAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkAC\nBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRCWMcbsDQBPyQUJEAQSIAgkQBBIgCCQAEEg\nAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkAChD+q\nNw2x3hxNggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585b9f2cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAABbCAYAAAARIRyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAW9JREFUeJzt27GxgUEYhlE/qpCbG0h0oTFF6UMTqrirAB4jMWvMOfEG\nb/TMl+wyxlgB8Gg9ewDAtxJIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSICwnT3glf/b/qf/QZ52\nx9kTPmpz+Js9AZ66XM/LO+9ckABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJ\nEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRA\nEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABB\nIAGCQAIEgQQIAgkQBBIgCCRAWMYYszcAfCUXJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiA\nIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRDuErEMsdeHDjkAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585aa640f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAABbCAYAAAARIRyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAXxJREFUeJzt28FpQkEYRlGf2IZ7W4kl2EyQFJB6bCBLsQrbGAuQGwLh\nMSLnrGfxrS7/ZpYxxgaAZ9vZAwBelUACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAoTd7AG/OXx+\nv/U/yP3Xz+wJq7rcb7MnrObjeJo9gX+4XM/LX965IAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSAB\ngkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQI\nAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAI\nJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAsIwxZm8AeEkuSIAgkABBIAGCQAIEgQQIAgkQ\nBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiA8ALqD\nD7GB5OuPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2585aa760b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME OVER\n",
      "Undiscounted Return : 500.56\n"
     ]
    }
   ],
   "source": [
    "undiscounted_return = 0\n",
    "\n",
    "game = make_BradtkeDomain(BRADTKE_DOMAIN)\n",
    "obs, reward, gamma = game.its_showtime()\n",
    "board_footprints = show_board(obs, leave_footprints=True)\n",
    "\n",
    "while undiscounted_return < 500:\n",
    "    obs, reward, gamma = game.play(0)\n",
    "    undiscounted_return += reward\n",
    "    board_footprints = show_board(obs, board_footprints, leave_footprints=True)\n",
    "print('GAME OVER')\n",
    "print('Undiscounted Return : {:.2f}'.format(undiscounted_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTD (in progess ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an implementation of LSTD($\\lambda$)\n",
    "# given: a simulaton model, featurizer, and $\\lambda$.\n",
    "# output: vector $\\beta$ so that $V^{\\pi}(x) = \\beta * \\phi(x)\n",
    "def _lstd(_func, _featurizer, _gamma, _lambda):\n",
    "    # set A = 0, b = 0, t = 0\n",
    "    A, b, t = 0, 0, 0\n",
    "    \n",
    "    #\n",
    "    for n in range(1000):\n",
    "        _model = _func()\n",
    "        \n",
    "        # choose start state $x_t$\n",
    "        obs, _, __ = _model.its_showtime()\n",
    "        position = np.array(obs.layers['P'])\n",
    "        state = np.unravel_index(position.argmax(), position.shape)\n",
    "        \n",
    "        # set $z_t = \\phi(x_t)$\n",
    "        eligibility = _featurizer(state[1])\n",
    "        \n",
    "        #\n",
    "        while not _model.game_over:\n",
    "            # simulate one step of the chain, producing a reward R_t and next state x_{t+1}\n",
    "            obs, reward, _ = _model.play(0)\n",
    "            position = np.array(obs.layers['P'])\n",
    "            next_state = np.unravel_index(position.argmax(), position.shape)\n",
    "            \n",
    "            #\n",
    "            delta = _featurizer(state[1]) - (_gamma * _featurizer(next_state[1]))\n",
    "            \n",
    "            # set $A = A + z_t \\cdot (\\phi(x_t) - \\phi(x_{t+1}))^T$\n",
    "            A = A + np.dot( eligibility, delta.T )\n",
    "            \n",
    "            # set $b = b + z_t * R_t$\n",
    "            b = b + (eligibility * reward)\n",
    "            \n",
    "            # set $z_{t+1} = lambda * z_t + \\phi(x_{t+1})$\n",
    "            eligibility = (_lambda * eligibility) + _featurizer(next_state[1])\n",
    "            \n",
    "            # set $t = t+1$\n",
    "            t = t + 1\n",
    "            \n",
    "            #\n",
    "            state = next_state\n",
    "        \n",
    "        # whenever padte coefficients are desired : set $\\beta = A^{-1} \\cdot b$ using SVD\n",
    "        A_ = np.linalg.pinv(A)\n",
    "        beta = np.dot(A_, b)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _r_lstd(_func, _featurizer, _gamma, _lambda):\n",
    "    \n",
    "    theta, C, t = np.zeros([4,1]), 1000*np.ones([4, 4]), 0\n",
    "    \n",
    "    for n in range(1000):\n",
    "        _model = _func()\n",
    "        \n",
    "        # choose start state $x_t$\n",
    "        obs, _, __ = _model.its_showtime()\n",
    "        position = np.array(obs.layers['P'])\n",
    "        state = np.unravel_index(position.argmax(), position.shape)\n",
    "        \n",
    "        # set $z_t = \\phi(x_t)$\n",
    "        eligibility = _featurizer(state[1])\n",
    "        \n",
    "        while not _model.game_over:\n",
    "            # simulate one step of the chain, producing a reward R_t and next state x_{t+1}\n",
    "            obs, reward, _ = _model.play(0)\n",
    "            position = np.array(obs.layers['P'])\n",
    "            next_state = np.unravel_index(position.argmax(), position.shape)\n",
    "            \n",
    "            #\n",
    "            delta = _featurizer(state[1]) - (_gamma * _featurizer(next_state[1]))\n",
    "            \n",
    "            #\n",
    "            L = np.dot( C, eligibility )\n",
    "            K = 1. / ( 1 + np.dot(delta.T, L) )\n",
    "            \n",
    "            #\n",
    "            e = reward - np.dot( delta.T, theta )\n",
    "            theta = theta +  ( K * np.dot( C, np.dot(eligibility, e) ) )\n",
    "            \n",
    "            #\n",
    "            C = C - (K * np.dot( C, np.dot( eligibility, np.dot(delta.T, C) ) ) )\n",
    "            \n",
    "            # set $z_{t+1} = lambda * z_t + \\phi(x_{t+1})$\n",
    "            eligibility = (_lambda * eligibility) + _featurizer(next_state[1])\n",
    "        \n",
    "            # set $t = t+1$\n",
    "            t = t + 1\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.57125082e+12]\n",
      " [ -2.57125082e+12]\n",
      " [ -2.57125082e+12]\n",
      " [ -2.57125082e+12]]\n"
     ]
    }
   ],
   "source": [
    "# LSTD(lambda)\n",
    "res = _lstd(lambda : make_BoyanChain(BOYAN_CHAIN), lambda x: boyan_features[x].reshape([-1, 1]), 1., .4)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-36676436.17896885]\n",
      " [-36676436.17896885]\n",
      " [-36676436.17896885]\n",
      " [-36676436.17896885]]\n"
     ]
    }
   ],
   "source": [
    "# rLSTD(lambda)\n",
    "res = _r_lstd(lambda : make_BoyanChain(BOYAN_CHAIN), lambda x: boyan_features[x].reshape([-1, 1]), 1., .4)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Equivalence of LSTD(1) and linear regression\n",
    "\n",
    "In his paper, Boyan demonstrates that in the case where $\\lambda=1$, the LSTD($\\lambda$) algorithm produces an approximation of the value function equivalent to the one we would obtain by off-line least-square linear regression. The linear regression method here is a simple supervised approach that learns to approximate the expected return for each state from Monte-Carlo returns of sampled trajectories.\n",
    "\n",
    "In this experiment we empirically verify this claim by numerically comparing the value function produced by those two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    \"\"\"\n",
    "    Samples a single trajectory and returns a training set (states, returns) that contain (x,y) values for each timestep\n",
    "    \"\"\"\n",
    "    game = make_BoyanChain(BOYAN_CHAIN)\n",
    "    obs, reward, gamma = game.its_showtime()\n",
    "\n",
    "    states = []\n",
    "    rewards = []\n",
    "    while not(game.game_over):    \n",
    "\n",
    "        # Stores current state (because it's not terminal)\n",
    "        agent_mask = np.array(obs.layers['P'])\n",
    "        position = np.unravel_index(agent_mask.argmax(), agent_mask.shape)\n",
    "        state = boyan_features[position[1]]\n",
    "        states.append(state)\n",
    "\n",
    "        # Do one transition and stores the reward\n",
    "        obs, reward, gamma = game.play(0)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    L = len(rewards)\n",
    "        \n",
    "    rewards = np.array(rewards)\n",
    "    returns = np.zeros_like(rewards)\n",
    "    states = np.array(states)\n",
    "\n",
    "    # Computes return for every timestep\n",
    "    for t in range(L):\n",
    "        returns[t] = np.sum(rewards[t:])\n",
    "    print(type(states))\n",
    "    \n",
    "    return states, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[ 1.3125  0.4375  0.      0.    ]\n",
      " [ 0.4375  1.6875  0.625   0.    ]\n",
      " [ 0.      0.625   1.5     0.375 ]\n",
      " [ 0.      0.      0.375   0.625 ]]\n",
      "[-37.25 -40.75 -19.25  -2.75]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "x, y = build_dataset()\n",
    "L = x.shape[0] # The lenghth of the trajectory\n",
    "\n",
    "A_LR = 0\n",
    "b_LR = 0\n",
    "for t in range(L):\n",
    "    A_LR += np.outer(x[t,:], x[t,:].T)\n",
    "    b_LR += x[t,:] * y[t]\n",
    "\n",
    "print(A_LR)\n",
    "print(b_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-23.06349206 -15.95238095  -5.98412698  -0.80952381]\n"
     ]
    }
   ],
   "source": [
    "A_inv = np.linalg.pinv(A_LR)\n",
    "beta = np.dot(A_inv, b_LR)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
